{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb67ba8d",
   "metadata": {},
   "source": [
    "# NLU: Mid-Term Assignment 2022\n",
    "### Description\n",
    "In this notebook, we ask you to complete four main tasks to show what you have learnt during the NLU labs. Therefore, to complete the assignment please refer to the concepts, libraries and other materials shown and used during the labs. The last task is not mandatory, it is a *BONUS* to get an extra mark for the laude. \n",
    "\n",
    "### Instructions\n",
    "- **Dataset**: in this notebook, you are asked to work with the dataset *Conll 2003* provided by us in the *data* folder. Please, load the files from the *data* folder and **do not** change names or paths of the inner files. \n",
    "- **Output**: for each part of your task, print your results and leave it in the notebook. Please, **do not** send a jupyter notebook without the printed outputs.\n",
    "- **Other**: follow carefully all the further instructions and suggestions given in the question descriptions.\n",
    "\n",
    "### Deadline\n",
    "The deadline is due in two weeks from the project presentation. Please, refer to *piazza* channel for the exact date."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3c05fb",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb069d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "from nltk.lm import Vocabulary\n",
    "from nltk.corpus import ConllCorpusReader\n",
    "\n",
    "CORPUS_ROOT = 'data'\n",
    "CORPUS_FILEIDS = ['train.txt', 'test.txt', 'valid.txt']\n",
    "CORPUS_COLUMNTYPES = ['words', 'ne', 'pos', 'chunk']\n",
    "\n",
    "corpus = ConllCorpusReader(CORPUS_ROOT, CORPUS_FILEIDS, CORPUS_COLUMNTYPES)\n",
    "corpus_train = ConllCorpusReader(CORPUS_ROOT, CORPUS_FILEIDS[0], CORPUS_COLUMNTYPES)\n",
    "corpus_test = ConllCorpusReader(CORPUS_ROOT, CORPUS_FILEIDS[1], CORPUS_COLUMNTYPES)\n",
    "corpus_val = ConllCorpusReader(CORPUS_ROOT, CORPUS_FILEIDS[2], CORPUS_COLUMNTYPES)\n",
    "\n",
    "# Utilities\n",
    "def nbest(d, n=1):\n",
    "    \"\"\"\n",
    "    get n max values from a dict\n",
    "    :param d: input dict (values are numbers, keys are stings)\n",
    "    :param n: number of values to get (int)\n",
    "    :return: dict of top n key-value pairs\n",
    "    \"\"\"\n",
    "    return dict(sorted(d.items(), key=lambda item: item[1], reverse=True)[:n])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996d123d",
   "metadata": {},
   "source": [
    "### Task 1: Analysis of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ead0d1f",
   "metadata": {},
   "source": [
    "#### Q 1.1\n",
    "- Create the Vocabulary and Frequency Dictionary of the:\n",
    "    1. Whole dataset\n",
    "    2. Train set\n",
    "    3. Test set\n",
    "    \n",
    "**Attention**: print the first 20 words of the Dictionaty of each set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca1124f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of whole dataset: 26869\n",
      "Length of train set: 21009\n",
      "Length of test set: 8548\n",
      "\n",
      "First 20 words of whole dataset:\n",
      "{'the': 12310, ',': 10876, '.': 10874, 'of': 5502, 'in': 5405, 'to': 5129, 'a': 4731, '(': 4226, ')': 4225, 'and': 4223, '\"': 3239, 'on': 3115, 'said': 2694, \"'s\": 2339, 'for': 2109, '-': 1866, '1': 1845, 'at': 1679, 'was': 1593, '2': 1342}\n",
      "\n",
      "First 20 words of train set:\n",
      "{'the': 8390, '.': 7374, ',': 7290, 'of': 3815, 'in': 3621, 'to': 3424, 'a': 3199, 'and': 2872, '(': 2861, ')': 2861, '\"': 2178, 'on': 2092, 'said': 1849, \"'s\": 1566, 'for': 1465, '1': 1421, '-': 1243, 'at': 1146, 'was': 1095, '2': 973}\n",
      "\n",
      "First 20 words of test set:\n",
      "{'the': 1765, ',': 1637, '.': 1626, 'to': 805, 'of': 789, 'in': 761, '(': 686, ')': 684, 'a': 658, 'and': 598, 'on': 467, '\"': 421, 'said': 399, \"'s\": 347, '-': 287, 'for': 286, 'at': 251, 'was': 224, '4': 201, 'with': 185}\n"
     ]
    }
   ],
   "source": [
    "def q11():\n",
    "    # Create vocabulary\n",
    "    vocab = set([w.lower() for w in corpus.words()])\n",
    "    vocab_train = set([w.lower() for w in corpus_train.words()])\n",
    "    vocab_test = set([w.lower() for w in corpus_test.words()])\n",
    "\n",
    "    # Create frequency distribution\n",
    "    fd = FreqDist([w.lower() for w in corpus.words()])\n",
    "    fd_train = FreqDist([w.lower() for w in corpus_train.words()])\n",
    "    fd_test = FreqDist([w.lower() for w in corpus_test.words()])\n",
    "\n",
    "    # Print vocabulary length\n",
    "    print(\"Length of whole dataset: %d\" % len(vocab))\n",
    "    print(\"Length of train set: %d\" % len(vocab_train))\n",
    "    print(\"Length of test set: %d\" % len(vocab_test))\n",
    "\n",
    "    # Print the first 20 words for each dict\n",
    "    print(\"\\nFirst 20 words of whole dataset:\")\n",
    "    print(nbest(fd, 20))\n",
    "    print(\"\\nFirst 20 words of train set:\")\n",
    "    print(nbest(fd_train, 20))\n",
    "    print(\"\\nFirst 20 words of test set:\")\n",
    "    print(nbest(fd_test, 20))\n",
    "\n",
    "q11()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0dc02f",
   "metadata": {},
   "source": [
    "#### Q 1.2\n",
    "- Obtain the list of:\n",
    "    1. Out-Of-Vocabulary (OOV) tokens\n",
    "    2. Overlapping tokens between train and test sets  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b660bcb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3268 OOV\n",
      "Found 5280 overlapping tokens\n"
     ]
    }
   ],
   "source": [
    "def q12(cutoff=1):\n",
    "    # Get vocabs\n",
    "    vocab_train = Vocabulary([w.lower() for w in corpus_train.words()], unk_cutoff=cutoff)\n",
    "    vocab_test = Vocabulary([w.lower() for w in corpus_test.words()], unk_cutoff=cutoff)\n",
    "\n",
    "    # Get list of tokens\n",
    "    tokens_train = set(vocab_train.counts.keys())\n",
    "    tokens_test = set(vocab_test.counts.keys())\n",
    "\n",
    "    # Get OOV\n",
    "    oov = tokens_test.difference(tokens_train)\n",
    "    print(\"Found {} OOV\".format(len(oov)))\n",
    "\n",
    "    # Get overlapping tokens\n",
    "    intersection = tokens_train.intersection(tokens_test)\n",
    "    print(\"Found {} overlapping tokens\".format(len(intersection)))\n",
    "    \n",
    "\n",
    "q12()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab1ac1c",
   "metadata": {},
   "source": [
    "#### Q 1.3\n",
    "- Perform a complete data analysis of the whole dataset (train + test sets) to obtain:\n",
    "    1. Average sentence length computed in number of tokens\n",
    "    2. The 50 most-common tokens\n",
    "    3. Number of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "36e5c39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sentence length: 13.6160\n",
      "50 most common tokens:\n",
      "the: 12310\n",
      ",: 10876\n",
      ".: 10874\n",
      "of: 5502\n",
      "in: 5405\n",
      "to: 5129\n",
      "a: 4731\n",
      "(: 4226\n",
      "): 4225\n",
      "and: 4223\n",
      "\": 3239\n",
      "on: 3115\n",
      "said: 2694\n",
      "'s: 2339\n",
      "for: 2109\n",
      "-: 1866\n",
      "1: 1845\n",
      "at: 1679\n",
      "was: 1593\n",
      "2: 1342\n",
      "with: 1267\n",
      "3: 1264\n",
      "0: 1232\n",
      "that: 1212\n",
      "he: 1166\n",
      "from: 1146\n",
      "by: 1113\n",
      "it: 1082\n",
      ":: 1057\n",
      "is: 984\n",
      "4: 973\n",
      "as: 920\n",
      "his: 867\n",
      "had: 841\n",
      "were: 804\n",
      "an: 796\n",
      "but: 786\n",
      "not: 786\n",
      "after: 780\n",
      "has: 768\n",
      "be: 754\n",
      "have: 738\n",
      "new: 656\n",
      "first: 645\n",
      "who: 643\n",
      "5: 636\n",
      "will: 591\n",
      "6: 584\n",
      "two: 579\n",
      "they: 567\n",
      "Number of sentences: 22137\n"
     ]
    }
   ],
   "source": [
    "def q13():\n",
    "\n",
    "    # Get average sentence length\n",
    "    print(\"Average sentence length: {:.4f}\".format(len(corpus.words())/len(corpus.sents())))\n",
    "\n",
    "    # Get 50 most common tokens\n",
    "    vocab = Vocabulary([w.lower() for w in corpus.words()])\n",
    "    most_common_tokens = nbest(vocab.counts, 50)\n",
    "    print(\"50 most common tokens:\")\n",
    "    for key in most_common_tokens:\n",
    "        print(\"{}: {}\".format(key, most_common_tokens[key]))\n",
    "\n",
    "    # Get number of sentences\n",
    "    print(\"Number of sentences: %d\" % len(corpus.sents()))\n",
    "\n",
    "q13()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726af097",
   "metadata": {},
   "source": [
    "#### Q 1.4\n",
    "- Create the dictionary of Named Entities and their Frequencies for the:\n",
    "    1. Whole dataset\n",
    "    2. Train set\n",
    "    3. Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5659670d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency dist of Named Entities for the whole dataset\n",
      " {'B-LOC': 10645, 'B-PER': 10059, 'B-ORG': 9323, 'I-PER': 6991, 'I-ORG': 5290, 'B-MISC': 5062, 'I-MISC': 1717, 'I-LOC': 1671}\n",
      "Frequency dist of Named Entities for the training set\n",
      " {'B-LOC': 7140, 'B-PER': 6600, 'B-ORG': 6321, 'I-PER': 4528, 'I-ORG': 3704, 'B-MISC': 3438, 'I-LOC': 1157, 'I-MISC': 1155}\n",
      "Frequency dist of Named Entities for the test set\n",
      " {'B-LOC': 1668, 'B-ORG': 1661, 'B-PER': 1617, 'I-PER': 1156, 'I-ORG': 835, 'B-MISC': 702, 'I-LOC': 257, 'I-MISC': 216}\n"
     ]
    }
   ],
   "source": [
    "def q14():\n",
    "    (WORD, POS, NE) = range(3)\n",
    "\n",
    "    # Whole dataset\n",
    "    ne_all = [w[NE] for w in corpus.iob_words() if w[NE] != 'O']\n",
    "    fd_all = FreqDist(ne_all)\n",
    "    print(\"Frequency dist of Named Entities for the whole dataset\\n\", nbest(fd_all, 20))\n",
    "\n",
    "    # Train set\n",
    "    ne_train = [w[NE] for w in corpus_train.iob_words() if w[NE] != 'O']\n",
    "    fd_train = FreqDist(ne_train)\n",
    "    print(\"Frequency dist of Named Entities for the training set\\n\", nbest(fd_train, 20))\n",
    "\n",
    "    # Whole dataset\n",
    "    ne_test = [w[NE] for w in corpus_test.iob_words() if w[NE] != 'O']\n",
    "    fd_test = FreqDist(ne_test)\n",
    "    print(\"Frequency dist of Named Entities for the test set\\n\", nbest(fd_test, 20))\n",
    "\n",
    "q14()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a08f37",
   "metadata": {},
   "source": [
    "### Task 2: Working with Dependecy Tree\n",
    "*Suggestions: use Spacy pipeline to retreive the Dependecy Tree*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1ba597",
   "metadata": {},
   "source": [
    "#### Q 2.1\n",
    "- Given each sentence in the dataset, write the required functions to provide:\n",
    "    1. Subject, obects (direct and indirect)\n",
    "    2. Noun chunks\n",
    "    3. The head noun in each noun chunk\n",
    "    \n",
    "**Attention**: *print only the results of these functions by using the sentence \"I saw the man with a telescope\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6292d5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "84838829",
   "metadata": {},
   "source": [
    "#### Q 2.2\n",
    "- Given a dependecy tree of a sentence and a segment of that sentence write the required functions that ouput the dependency subtree of that segment.\n",
    "\n",
    "**Attention**: *print only the results of these functions by using the sentence \"I saw the man with a telescope\" (the segment could be any e.g. \"saw the man\", \"a telescope\", etc.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d524e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "292e99ac",
   "metadata": {},
   "source": [
    "#### Q 2.3\n",
    "- Given a token in a sentence, write the required functions that output the dependency path from the root of the dependency tree to that given token.\n",
    "\n",
    "**Attention**: *print only the results of these functions by using the sentence \"I saw the man with a telescope\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0b1106",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b3358779",
   "metadata": {},
   "source": [
    "### Task 3: Named Entity Recognition\n",
    "*Suggestion: use scikit-learn metric functions. See classification_report*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c820ad69",
   "metadata": {},
   "source": [
    "#### Q 3.1\n",
    "- Benchmark Spacy Named Entity Recognition model on the test set by:\n",
    "    1. Providing the list of categories in the dataset (person, organization, etc.)\n",
    "    2. Computing the overall accuracy on NER\n",
    "    3. Computing the performance of the Named Entity Recognition model for each category:\n",
    "        - Compute the perfomance at the token level (eg. B-Person, I-Person, B-Organization, I-Organization, O, etc.)\n",
    "        - Compute the performance at the entity level (eg. Person, Organization, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a051c93d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d669ee84",
   "metadata": {},
   "source": [
    "### Task 4: BONUS PART (extra mark for laude)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f56fc4f",
   "metadata": {},
   "source": [
    "#### Q 4.1\n",
    "- Modify NLTK Transition parser's Configuration calss to use better features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b182ed9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "41ebf011",
   "metadata": {},
   "source": [
    "#### Q 4.2\n",
    "- Evaluate the features comparing performance to the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5177f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cfa4657c",
   "metadata": {},
   "source": [
    "#### Q 4.3\n",
    "- Replace SVM classifier with an alternative of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b94966",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
