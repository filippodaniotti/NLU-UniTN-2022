{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ac00029",
   "metadata": {},
   "source": [
    "# NLU: Mid-Term Assignment 2022\n",
    "### Description\n",
    "In this notebook, we ask you to complete four main tasks to show what you have learnt during the NLU labs. Therefore, to complete the assignment please refer to the concepts, libraries and other materials shown and used during the labs. The last task is not mandatory, it is a *BONUS* to get an extra mark for the laude. \n",
    "\n",
    "### Instructions\n",
    "- **Dataset**: in this notebook, you are asked to work with the dataset *Conll 2003* provided by us in the *data* folder. Please, load the files from the *data* folder and **do not** change names or paths of the inner files. \n",
    "- **Output**: for each part of your task, print your results and leave it in the notebook. Please, **do not** send a jupyter notebook without the printed outputs.\n",
    "- **Other**: follow carefully all the further instructions and suggestions given in the question descriptions.\n",
    "\n",
    "### Deadline\n",
    "The deadline is due in two weeks from the project presentation. Please, refer to *piazza* channel for the exact date."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e1f620",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ba6c92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "from nltk.lm import Vocabulary\n",
    "from nltk.corpus import ConllCorpusReader\n",
    "\n",
    "CORPUS_ROOT = 'data'\n",
    "CORPUS_FILEIDS = ['train.txt', 'test.txt', 'valid.txt']\n",
    "CORPUS_COLUMNTYPES = ['words', 'ne', 'pos', 'chunk', 'tree']\n",
    "\n",
    "corpus = ConllCorpusReader(CORPUS_ROOT, CORPUS_FILEIDS, CORPUS_COLUMNTYPES)\n",
    "corpus_train = ConllCorpusReader(CORPUS_ROOT, CORPUS_FILEIDS[0], CORPUS_COLUMNTYPES)\n",
    "corpus_test = ConllCorpusReader(CORPUS_ROOT, CORPUS_FILEIDS[1], CORPUS_COLUMNTYPES)\n",
    "corpus_val = ConllCorpusReader(CORPUS_ROOT, CORPUS_FILEIDS[2], CORPUS_COLUMNTYPES)\n",
    "\n",
    "# Removing DOCSTART and other empty lists\n",
    "corpus_sents = [s for s in corpus.sents() if s != []]\n",
    "corpus_train_sents = [s for s in corpus_train.sents() if s != []]\n",
    "corpus_test_sents = [s for s in corpus_test.sents() if s != []]\n",
    "corpus_val_sents = [s for s in corpus_val.sents() if s != []]\n",
    "\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.tokenizer = Tokenizer(nlp.vocab)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath('./src/'))\n",
    "from conll import evaluate\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Utilities\n",
    "def nbest(d, n=1):\n",
    "    \"\"\"\n",
    "    get n max values from a dict\n",
    "    :param d: input dict (values are numbers, keys are stings)\n",
    "    :param n: number of values to get (int)\n",
    "    :return: dict of top n key-value pairs\n",
    "    \"\"\"\n",
    "    return dict(sorted(d.items(), key=lambda item: item[1], reverse=True)[:n])\n",
    "\n",
    "def get_flat_sents(corpus):\n",
    "    sents = list()\n",
    "    for sent in corpus:\n",
    "        flat_sent = \"\"\n",
    "        for w in sent:\n",
    "            flat_sent += f\"{w} \"\n",
    "        sents.append(flat_sent.strip())\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f40e1f",
   "metadata": {},
   "source": [
    "## Task 1: Analysis of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18406477",
   "metadata": {},
   "source": [
    "### Q 1.1\n",
    "- Create the Vocabulary and Frequency Dictionary of the:\n",
    "    1. Whole dataset\n",
    "    2. Train set\n",
    "    3. Test set\n",
    "    \n",
    "**Attention**: print the first 20 words of the Dictionaty of each set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "811cdb47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of whole dataset: 26869\n",
      "Length of train set: 21009\n",
      "Length of test set: 8548\n",
      "\n",
      "First 20 words of whole dataset:\n",
      "{'the': 12310, ',': 10876, '.': 10874, 'of': 5502, 'in': 5405, 'to': 5129, 'a': 4731, '(': 4226, ')': 4225, 'and': 4223, '\"': 3239, 'on': 3115, 'said': 2694, \"'s\": 2339, 'for': 2109, '-': 1866, '1': 1845, 'at': 1679, 'was': 1593, '2': 1342}\n",
      "\n",
      "First 20 words of train set:\n",
      "{'the': 8390, '.': 7374, ',': 7290, 'of': 3815, 'in': 3621, 'to': 3424, 'a': 3199, 'and': 2872, '(': 2861, ')': 2861, '\"': 2178, 'on': 2092, 'said': 1849, \"'s\": 1566, 'for': 1465, '1': 1421, '-': 1243, 'at': 1146, 'was': 1095, '2': 973}\n",
      "\n",
      "First 20 words of test set:\n",
      "{'the': 1765, ',': 1637, '.': 1626, 'to': 805, 'of': 789, 'in': 761, '(': 686, ')': 684, 'a': 658, 'and': 598, 'on': 467, '\"': 421, 'said': 399, \"'s\": 347, '-': 287, 'for': 286, 'at': 251, 'was': 224, '4': 201, 'with': 185}\n"
     ]
    }
   ],
   "source": [
    "def q11():\n",
    "    # Create vocabulary\n",
    "    vocab = set([w.lower() for w in corpus.words()])\n",
    "    vocab_train = set([w.lower() for w in corpus_train.words()])\n",
    "    vocab_test = set([w.lower() for w in corpus_test.words()])\n",
    "\n",
    "    # Create frequency distribution\n",
    "    fd = FreqDist([w.lower() for w in corpus.words()])\n",
    "    fd_train = FreqDist([w.lower() for w in corpus_train.words()])\n",
    "    fd_test = FreqDist([w.lower() for w in corpus_test.words()])\n",
    "\n",
    "    # Print vocabulary length\n",
    "    print(\"Length of whole dataset: %d\" % len(vocab))\n",
    "    print(\"Length of train set: %d\" % len(vocab_train))\n",
    "    print(\"Length of test set: %d\" % len(vocab_test))\n",
    "\n",
    "    # Print the first 20 words for each dict\n",
    "    print(\"\\nFirst 20 words of whole dataset:\")\n",
    "    print(nbest(fd, 20))\n",
    "    print(\"\\nFirst 20 words of train set:\")\n",
    "    print(nbest(fd_train, 20))\n",
    "    print(\"\\nFirst 20 words of test set:\")\n",
    "    print(nbest(fd_test, 20))\n",
    "\n",
    "q11()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6e75cf",
   "metadata": {},
   "source": [
    "### Q 1.2\n",
    "- Obtain the list of:\n",
    "    1. Out-Of-Vocabulary (OOV) tokens\n",
    "    2. Overlapping tokens between train and test sets  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "01dd3358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Q1.2.1]\n",
      ">\tOOV tokens:\n",
      ">\t (test) Found 3268 OOV\n",
      ">\t (valid) Found 2856 OOV\n",
      ">\t (test + valid) Found 0 OOV\n",
      "\n",
      "[Q1.2.1]\n",
      ">\tOverlapping tokens:\n",
      ">\t (test) Found 5280 overlapping tokens\n",
      ">\t (valid) Found 6146 overlapping tokens\n",
      ">\t (test + val) Found 8066 overlapping tokens\n"
     ]
    }
   ],
   "source": [
    "def q12(cutoff=1):\n",
    "\n",
    "\n",
    "    test_lower = [w.lower() for w in corpus_test.words()]\n",
    "    val_lower = [w.lower() for w in corpus_val.words()]\n",
    "    # Get vocabs\n",
    "    vocab_train = Vocabulary([w.lower() for w in corpus_train.words()], unk_cutoff=cutoff)\n",
    "    vocab_test = Vocabulary(test_lower, unk_cutoff=cutoff)\n",
    "    vocab_valid = Vocabulary(val_lower, unk_cutoff=cutoff)\n",
    "    vocab_tv = Vocabulary([*test_lower, *val_lower], unk_cutoff=cutoff)\n",
    "\n",
    "    # Get list of tokens\n",
    "    tokens_train = set(vocab_train.counts.keys())\n",
    "    tokens_test = set(vocab_test.counts.keys())\n",
    "    tokens_val = set(vocab_valid.counts.keys())\n",
    "    tokens_tv = set(vocab_tv.counts.keys())\n",
    "\n",
    "    # Get OOV \n",
    "    oov_test = tokens_test.difference(tokens_train)\n",
    "    oov_valid = tokens_val.difference(tokens_train)\n",
    "    oov_tv = tokens_val.difference(tokens_tv)\n",
    "    print(\"[Q1.2.1]\\n>\\tOOV tokens:\")\n",
    "    print(\">\\t (test) Found {} OOV\".format(len(oov_test)))\n",
    "    print(\">\\t (valid) Found {} OOV\".format(len(oov_valid)))\n",
    "    print(\">\\t (test + valid) Found {} OOV\".format(len(oov_tv)))\n",
    "\n",
    "    print()\n",
    "\n",
    "    # Get overlapping tokens w/ test set\n",
    "    intersection_test = tokens_train.intersection(tokens_test)\n",
    "    intersection_val = tokens_train.intersection(tokens_val)\n",
    "    intersection_tv = tokens_train.intersection(tokens_tv)\n",
    "    print(\"[Q1.2.1]\\n>\\tOverlapping tokens:\")\n",
    "    print(\">\\t (test) Found {} overlapping tokens\".format(len(intersection_test)))\n",
    "    print(\">\\t (valid) Found {} overlapping tokens\".format(len(intersection_val)))\n",
    "    print(\">\\t (test + val) Found {} overlapping tokens\".format(len(intersection_tv)))\n",
    "    \n",
    "\n",
    "q12()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0f8734",
   "metadata": {},
   "source": [
    "### Q 1.3\n",
    "- Perform a complete data analysis of the whole dataset (train + test sets) to obtain:\n",
    "    1. Average sentence length computed in number of tokens\n",
    "    2. The 50 most-common tokens\n",
    "    3. Number of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b7213d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Q1.3.1]\n",
      ">\tAverage sentence length in tokens: 14.5304\n",
      "\n",
      "[Q1.3.2]\n",
      ">\t50 most common tokens:\n",
      ">\t[1] the: 12310\n",
      ">\t[2] ,: 10876\n",
      ">\t[3] .: 10874\n",
      ">\t[4] of: 5502\n",
      ">\t[5] in: 5405\n",
      ">\t[6] to: 5129\n",
      ">\t[7] a: 4731\n",
      ">\t[8] (: 4226\n",
      ">\t[9] ): 4225\n",
      ">\t[10] and: 4223\n",
      ">\t[11] \": 3239\n",
      ">\t[12] on: 3115\n",
      ">\t[13] said: 2694\n",
      ">\t[14] 's: 2339\n",
      ">\t[15] for: 2109\n",
      ">\t[16] -: 1866\n",
      ">\t[17] 1: 1845\n",
      ">\t[18] at: 1679\n",
      ">\t[19] was: 1593\n",
      ">\t[20] 2: 1342\n",
      ">\t[21] with: 1267\n",
      ">\t[22] 3: 1264\n",
      ">\t[23] 0: 1232\n",
      ">\t[24] that: 1212\n",
      ">\t[25] he: 1166\n",
      ">\t[26] from: 1146\n",
      ">\t[27] by: 1113\n",
      ">\t[28] it: 1082\n",
      ">\t[29] :: 1057\n",
      ">\t[30] is: 984\n",
      ">\t[31] 4: 973\n",
      ">\t[32] as: 920\n",
      ">\t[33] his: 867\n",
      ">\t[34] had: 841\n",
      ">\t[35] were: 804\n",
      ">\t[36] an: 796\n",
      ">\t[37] but: 786\n",
      ">\t[38] not: 786\n",
      ">\t[39] after: 780\n",
      ">\t[40] has: 768\n",
      ">\t[41] be: 754\n",
      ">\t[42] have: 738\n",
      ">\t[43] new: 656\n",
      ">\t[44] first: 645\n",
      ">\t[45] who: 643\n",
      ">\t[46] 5: 636\n",
      ">\t[47] will: 591\n",
      ">\t[48] 6: 584\n",
      ">\t[49] two: 579\n",
      ">\t[50] they: 567\n",
      "\n",
      "[Q1.3.3]\n",
      ">\tNumber of sentences: 20744\n"
     ]
    }
   ],
   "source": [
    "def q13():\n",
    "\n",
    "    # Get average sentence length\n",
    "    print(\"[Q1.3.1]\\n>\\tAverage sentence length in tokens: {:.4f}\\n\".format(len(corpus.words())/len(corpus_sents)))\n",
    "\n",
    "    # Get 50 most common tokens\n",
    "    vocab = Vocabulary([w.lower() for w in corpus.words()])\n",
    "    most_common_tokens = nbest(vocab.counts, 50)\n",
    "    print(\"[Q1.3.2]\\n>\\t50 most common tokens:\")\n",
    "    # print(\">\\t\", most_common_tokens)\n",
    "    count = 1\n",
    "    for key in most_common_tokens:\n",
    "        print(\">\\t[{}] {}: {}\".format(count, key, most_common_tokens[key]))\n",
    "        count += 1\n",
    "\n",
    "    # Get number of sentences\n",
    "    print(\"\\n[Q1.3.3]\\n>\\tNumber of sentences: %d\" % len(corpus_sents))\n",
    "\n",
    "q13()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9fb7a7",
   "metadata": {},
   "source": [
    "### Q 1.4\n",
    "- Create the dictionary of Named Entities and their Frequencies for the:\n",
    "    1. Whole dataset\n",
    "    2. Train set\n",
    "    3. Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f35a5b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Q1.4.1]\n",
      ">\tFrequency dist of Named Entities for the whole dataset\n",
      ">\t {'U.S.': 460, 'Germany': 237, 'Australia': 204, 'France': 199, 'England': 176, 'Russia': 167, 'Britain': 165, 'Italy': 160, 'China': 149, 'LONDON': 147, 'Spain': 145, 'NEW YORK': 143, 'Japan': 133, 'Russian': 120, 'German': 114, 'Reuters': 114, 'Israel': 108, 'Sweden': 108, 'Pakistan': 103, 'Iraq': 98}\n",
      "[Q1.4.2]\n",
      ">\tFrequency dist of Named Entities for the training set\n",
      ">\t {'U.S.': 303, 'Germany': 141, 'Britain': 133, 'Australia': 130, 'England': 123, 'France': 122, 'Spain': 110, 'Italy': 98, 'NEW YORK': 95, 'LONDON': 93, 'Russian': 92, 'China': 91, 'Russia': 88, 'Japan': 87, 'Pakistan': 85, 'Sweden': 81, 'German': 80, 'British': 73, 'Reuters': 73, 'Belgium': 71}\n",
      "[Q1.4.3]\n",
      ">\tFrequency dist of Named Entities for the test set\n",
      ">\t {'Germany': 49, 'U.S.': 45, 'Australia': 45, 'Japan': 41, 'Italy': 41, 'France': 40, 'World Cup': 34, 'Russia': 34, 'Indonesia': 33, 'China': 32, 'LONDON': 31, 'Austria': 29, 'Barcelona': 24, 'Canada': 23, 'NEW YORK': 22, 'England': 21, 'Singapore': 21, 'Poland': 20, 'Reuters': 19, 'WTO': 19}\n"
     ]
    }
   ],
   "source": [
    "def q14():\n",
    "    WORD, _, NE = range(3)\n",
    "\n",
    "    def merge_iob_tags(doc):\n",
    "        idx = 0\n",
    "        merged_ne = list()\n",
    "        for idx in range(len(doc)):\n",
    "            if doc[idx][NE].split('-')[0] == \"B\":\n",
    "                temp = str(doc[idx][WORD])\n",
    "                idx += 1\n",
    "                while idx < len(doc) and doc[idx][NE].split('-')[0] == \"I\":\n",
    "                    temp += \" %s\" % str(doc[idx][WORD])\n",
    "                    idx += 1\n",
    "                merged_ne.append(temp)\n",
    "        return merged_ne\n",
    "\n",
    "    # Whole dataset\n",
    "    iob_all = [(w[WORD], _, w[NE]) for w in corpus.iob_words() if w[NE] != 'O']\n",
    "    ne_all = merge_iob_tags(iob_all)\n",
    "    fd_all = FreqDist(ne_all)\n",
    "    print(\"[Q1.4.1]\\n>\\tFrequency dist of Named Entities for the whole dataset\\n>\\t\", nbest(fd_all, 20))\n",
    "\n",
    "    # Train set\n",
    "    iob_train = [(w[WORD], _, w[NE]) for w in corpus_train.iob_words() if w[NE] != 'O']\n",
    "    ne_train = merge_iob_tags(iob_train)\n",
    "    fd_train = FreqDist(ne_train)\n",
    "    print(\"[Q1.4.2]\\n>\\tFrequency dist of Named Entities for the training set\\n>\\t\", nbest(fd_train, 20))\n",
    "\n",
    "    # Test set\n",
    "    iob_test = [(w[WORD], _, w[NE]) for w in corpus_test.iob_words() if w[NE] != 'O']\n",
    "    ne_test = merge_iob_tags(iob_test)\n",
    "    fd_test = FreqDist(ne_test)\n",
    "    print(\"[Q1.4.3]\\n>\\tFrequency dist of Named Entities for the test set\\n>\\t\", nbest(fd_test, 20))\n",
    "\n",
    "q14()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed42c93",
   "metadata": {},
   "source": [
    "## Task 2: Working with Dependecy Tree\n",
    "*Suggestions: use Spacy pipeline to retreive the Dependecy Tree*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76d1903",
   "metadata": {},
   "source": [
    "### Q 2.1\n",
    "- Given each sentence in the dataset, write the required functions to provide:\n",
    "    1. Subject, obects (direct and indirect)\n",
    "    2. Noun chunks\n",
    "    3. The head noun in each noun chunk\n",
    "    \n",
    "**Attention**: *print only the results of these functions by using the sentence \"I saw the man with a telescope\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0b3b578e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I saw the man with a telescope \n",
      "\n",
      "[Q2.1.1]\n",
      ">\tProviding subjects and objects:\n",
      ">\t nsubj: ['I']\n",
      ">\t dobj: ['man']\n",
      ">\t pobj: ['telescope']\n",
      "\n",
      "[Q2.1.2]\n",
      ">\tProviding noun chunks:\n",
      ">\t I\n",
      ">\t the man\n",
      ">\t a telescope\n",
      "\n",
      "[Q2.1.3]\n",
      ">\tProviding head noun for each noun chunk:\n",
      ">\t'CHUNK' -> HEAD\n",
      ">\n",
      ">\t'I' -> I\n",
      ">\t'the man' -> man\n",
      ">\t'a telescope' -> telescope\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def q21(corpus):\n",
    "\n",
    "    def get_subj_obj_dict(doc):\n",
    "        deps_dict = dict()\n",
    "        deps = ['nsubj', 'dobj', 'pobj']\n",
    "        for dep in deps:\n",
    "            deps_dict[dep] = list()\n",
    "        for token in doc:\n",
    "            if token.dep_ in deps:\n",
    "                deps_dict[token.dep_].append(token.text)\n",
    "        return deps_dict\n",
    "                    \n",
    "    def get_noun_chunks(doc):\n",
    "        return doc.noun_chunks\n",
    "\n",
    "    def get_head_of_chunk(doc):\n",
    "        return [(c.root.text, c.text) for c in doc.noun_chunks]\n",
    "\n",
    "\n",
    "    def q211(doc):\n",
    "        print(\"[Q2.1.1]\\n>\\tProviding subjects and objects:\")\n",
    "        deps_dict = get_subj_obj_dict(doc)\n",
    "        for key in deps_dict:\n",
    "            print(\">\\t {}: {}\".format(key, deps_dict[key]))\n",
    "        print()\n",
    "\n",
    "\n",
    "    def q212(doc):\n",
    "        print(\"[Q2.1.2]\\n>\\tProviding noun chunks:\")\n",
    "        noun_chunks = get_noun_chunks(doc)\n",
    "        for chunk in noun_chunks:\n",
    "            print(\">\\t\", chunk)\n",
    "        print()\n",
    "\n",
    "    def q213(doc):\n",
    "        print(\"[Q2.1.3]\\n>\\tProviding head noun for each noun chunk:\")\n",
    "        print(\">\\t'CHUNK' -> HEAD\\n>\")\n",
    "        heads = get_head_of_chunk(doc)\n",
    "        for head, chunk in heads:\n",
    "            print(\">\\t'{}' -> {}\".format(chunk, head))\n",
    "        print()\n",
    "\n",
    "\n",
    "    sents = get_flat_sents(corpus)\n",
    "    for sent in sents[:10]:\n",
    "        doc = nlp(sent)\n",
    "        print (sent, \"\\n\")\n",
    "        q211(doc)\n",
    "        q212(doc)\n",
    "        q213(doc)\n",
    "\n",
    "    # doc = nlp(sents[14])\n",
    "    # print (sents[14], \"\\n\")\n",
    "    # q211(doc)\n",
    "    # q212(doc)\n",
    "    # q213(doc)\n",
    "\n",
    "\n",
    "q21([\"I saw the man with a telescope\".split(\" \")])\n",
    "# q21(corpus_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a103b89e",
   "metadata": {},
   "source": [
    "### Q 2.2\n",
    "- Given a dependecy tree of a sentence and a segment of that sentence write the required functions that ouput the dependency subtree of that segment.\n",
    "\n",
    "**Attention**: *print only the results of these functions by using the sentence \"I saw the man with a telescope\" (the segment could be any e.g. \"saw the man\", \"a telescope\", etc.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8ae3c686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I saw the man with the telescope\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"dda8f45baac941a7a7b7fb45854e66d4-0\" class=\"displacy\" width=\"1275\" height=\"312.0\" direction=\"ltr\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">I</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">saw</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">man</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">with</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">telescope</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-dda8f45baac941a7a7b7fb45854e66d4-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,89.5 220.0,89.5 220.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-dda8f45baac941a7a7b7fb45854e66d4-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-dda8f45baac941a7a7b7fb45854e66d4-0-1\" stroke-width=\"2px\" d=\"M420,177.0 C420,89.5 570.0,89.5 570.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-dda8f45baac941a7a7b7fb45854e66d4-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,179.0 L412,167.0 428,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-dda8f45baac941a7a7b7fb45854e66d4-0-2\" stroke-width=\"2px\" d=\"M245,177.0 C245,2.0 575.0,2.0 575.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-dda8f45baac941a7a7b7fb45854e66d4-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M575.0,179.0 L583.0,167.0 567.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-dda8f45baac941a7a7b7fb45854e66d4-0-3\" stroke-width=\"2px\" d=\"M595,177.0 C595,89.5 745.0,89.5 745.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-dda8f45baac941a7a7b7fb45854e66d4-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M745.0,179.0 L753.0,167.0 737.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-dda8f45baac941a7a7b7fb45854e66d4-0-4\" stroke-width=\"2px\" d=\"M945,177.0 C945,89.5 1095.0,89.5 1095.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-dda8f45baac941a7a7b7fb45854e66d4-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M945,179.0 L937,167.0 953,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-dda8f45baac941a7a7b7fb45854e66d4-0-5\" stroke-width=\"2px\" d=\"M770,177.0 C770,2.0 1100.0,2.0 1100.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-dda8f45baac941a7a7b7fb45854e66d4-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1100.0,179.0 L1108.0,167.0 1092.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk: I\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"2b2d51fcdc944c58b7222da9e43b7289-0\" class=\"displacy\" width=\"225\" height=\"137.0\" direction=\"ltr\" style=\"max-width: none; height: 137.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"47.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">I</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
       "</text>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk: the man\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"71a03a0479a345298f99caf18fb459d0-0\" class=\"displacy\" width=\"400\" height=\"224.5\" direction=\"ltr\" style=\"max-width: none; height: 224.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">man</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-71a03a0479a345298f99caf18fb459d0-0-0\" stroke-width=\"2px\" d=\"M70,89.5 C70,2.0 225.0,2.0 225.0,89.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-71a03a0479a345298f99caf18fb459d0-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,91.5 L62,79.5 78,79.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk: the telescope\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"b5f19535a71c40aabae7a2bef5a8515f-0\" class=\"displacy\" width=\"400\" height=\"224.5\" direction=\"ltr\" style=\"max-width: none; height: 224.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">telescope</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-b5f19535a71c40aabae7a2bef5a8515f-0-0\" stroke-width=\"2px\" d=\"M70,89.5 C70,2.0 225.0,2.0 225.0,89.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-b5f19535a71c40aabae7a2bef5a8515f-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,91.5 L62,79.5 78,79.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def q22(corpus):\n",
    "\n",
    "    def get_root(doc):\n",
    "        for token in doc:\n",
    "            if token.dep_ == 'ROOT':\n",
    "                return token.text \n",
    "            else:\n",
    "                continue\n",
    "\n",
    "    def get_subtree(chunk_text, doc):\n",
    "        subtree = None\n",
    "        chunk_doc = nlp(chunk_text)\n",
    "        chunk_root = get_root(chunk_doc)\n",
    "        for token in doc:\n",
    "            if token.text == chunk_root:\n",
    "                leftmost = list(token.subtree)[0].i\n",
    "                rightmost = list(token.subtree)[-1].i\n",
    "                subtree = doc[leftmost:rightmost+1]\n",
    "        return subtree\n",
    "\n",
    "\n",
    "    sents = get_flat_sents(corpus)\n",
    "    # sents = [sents[14]]\n",
    "    for sent in sents:\n",
    "        print(sent)\n",
    "        doc = nlp(sent)\n",
    "        spacy.displacy.render(doc, style=\"dep\")\n",
    "        for chunk in doc.noun_chunks:\n",
    "            # subtree = get_subtree(chunk.text, doc)\n",
    "            # print(\"Chunk: {}\\n>\\tSubtree: {}\".format(chunk, chunk))\n",
    "            print(\"Chunk: {}\".format(chunk))\n",
    "            spacy.displacy.render(chunk, style=\"dep\")\n",
    "    \n",
    "q22([\n",
    "    \"I saw the man with the telescope\".split(\" \"),\n",
    "    # \"I saw the man with a telescope\".split(\" \")\n",
    "])\n",
    "# q22(corpus_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef7be6e",
   "metadata": {},
   "source": [
    "### Q 2.3\n",
    "- Given a token in a sentence, write the required functions that output the dependency path from the root of the dependency tree to that given token.\n",
    "\n",
    "**Attention**: *print only the results of these functions by using the sentence \"I saw the man with a telescope\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a2fd0e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I saw the man with a telescope\n",
      "TOKEN ---> ['path', 'to', 'token']\n",
      "\n",
      "I\n",
      "\t---> ['saw', 'I']\n",
      "saw\n",
      "\t---> ['saw']\n",
      "the\n",
      "\t---> ['saw', 'man', 'the']\n",
      "man\n",
      "\t---> ['saw', 'man']\n",
      "with\n",
      "\t---> ['saw', 'with']\n",
      "a\n",
      "\t---> ['saw', 'with', 'telescope', 'a']\n",
      "telescope\n",
      "\t---> ['saw', 'with', 'telescope']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"74eddd29a20d4c75bd31d368fd7c1df8-0\" class=\"displacy\" width=\"1275\" height=\"399.5\" direction=\"ltr\" style=\"max-width: none; height: 399.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">I</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">saw</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">man</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">with</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">a</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">telescope</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-74eddd29a20d4c75bd31d368fd7c1df8-0-0\" stroke-width=\"2px\" d=\"M70,264.5 C70,177.0 215.0,177.0 215.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-74eddd29a20d4c75bd31d368fd7c1df8-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,266.5 L62,254.5 78,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-74eddd29a20d4c75bd31d368fd7c1df8-0-1\" stroke-width=\"2px\" d=\"M420,264.5 C420,177.0 565.0,177.0 565.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-74eddd29a20d4c75bd31d368fd7c1df8-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,266.5 L412,254.5 428,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-74eddd29a20d4c75bd31d368fd7c1df8-0-2\" stroke-width=\"2px\" d=\"M245,264.5 C245,89.5 570.0,89.5 570.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-74eddd29a20d4c75bd31d368fd7c1df8-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M570.0,266.5 L578.0,254.5 562.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-74eddd29a20d4c75bd31d368fd7c1df8-0-3\" stroke-width=\"2px\" d=\"M245,264.5 C245,2.0 750.0,2.0 750.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-74eddd29a20d4c75bd31d368fd7c1df8-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M750.0,266.5 L758.0,254.5 742.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-74eddd29a20d4c75bd31d368fd7c1df8-0-4\" stroke-width=\"2px\" d=\"M945,264.5 C945,177.0 1090.0,177.0 1090.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-74eddd29a20d4c75bd31d368fd7c1df8-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M945,266.5 L937,254.5 953,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-74eddd29a20d4c75bd31d368fd7c1df8-0-5\" stroke-width=\"2px\" d=\"M770,264.5 C770,89.5 1095.0,89.5 1095.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-74eddd29a20d4c75bd31d368fd7c1df8-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1095.0,266.5 L1103.0,254.5 1087.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def q23(corpus: list, cut=5):\n",
    "        # print()\n",
    "    def compute_dependency_path(token):\n",
    "        path = list()\n",
    "        path.append(token.text)\n",
    "        while token.dep_ != 'ROOT':\n",
    "            token = token.head\n",
    "            path.append(token.text)\n",
    "        path.reverse()\n",
    "        return path\n",
    "\n",
    "    if len(corpus) >= cut:\n",
    "        corpus = corpus[:cut]\n",
    "    sents = get_flat_sents(corpus)\n",
    "    for sent in sents:\n",
    "        print(sent)\n",
    "        doc = nlp(sent)\n",
    "        print(\"TOKEN ---> ['path', 'to', 'token']\\n\")\n",
    "        for token in doc:\n",
    "            print(\"{}\\n\\t---> {}\".format(token.text, compute_dependency_path(token)))\n",
    "\n",
    "    spacy.displacy.render(doc, style=\"dep\")\n",
    "\n",
    "q23([\"I saw the man with a telescope\".split(\" \")], 10)\n",
    "# q23(corpus_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece505f0",
   "metadata": {},
   "source": [
    "## Task 3: Named Entity Recognition\n",
    "*Suggestion: use scikit-learn metric functions. See classification_report*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27df64cf",
   "metadata": {},
   "source": [
    "### Q 3.1\n",
    "- Benchmark Spacy Named Entity Recognition model on the test set by:\n",
    "    1. Providing the list of categories in the dataset (person, organization, etc.)\n",
    "    2. Computing the overall accuracy on NER\n",
    "    3. Computing the performance of the Named Entity Recognition model for each category:\n",
    "        - Compute the perfomance at the token level (eg. B-Person, I-Person, B-Organization, I-Organization, O, etc.)\n",
    "        - Compute the performance at the entity level (eg. Person, Organization, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "24381fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting processed dataset\n",
    "sents_train = get_flat_sents(corpus_train.sents())\n",
    "docs_train = [nlp(sent) for sent in sents_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a6dd28fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_test = get_flat_sents(corpus_test.sents())\n",
    "docs_test = [nlp(sent) for sent in sents_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2a49d3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_all = get_flat_sents(corpus.sents())\n",
    "docs_all = [nlp(sent) for sent in sents_all]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf1d98e",
   "metadata": {},
   "source": [
    "#### Q 3.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "89f3feac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "def get_categories(corpus: ConllCorpusReader):\n",
    "    categories = set()\n",
    "    for token in chain.from_iterable(corpus._grids()):\n",
    "        ent_type = token[3]\n",
    "        if ent_type != 'O':\n",
    "            categories.add(ent_type.split('-')[1])\n",
    "    return list(categories)\n",
    "\n",
    "def get_categories_spacy(docs):\n",
    "    categories = set()\n",
    "    for doc in docs:\n",
    "        # doc = nlp(sent)\n",
    "        for token in doc:\n",
    "            categories.add(token.ent_type_)\n",
    "        if '' in categories:\n",
    "            categories.discard('')\n",
    "    return list(categories)\n",
    "\n",
    "def q311(corpus: ConllCorpusReader, docs: spacy):\n",
    "    print(\"[Q3.1.1]\\n>\\tProviding list of categories in the dataset:\")\n",
    "    print(\">\\t Original dataset:\", end=\"\\n>\\t\\t\")\n",
    "    categories = get_categories(corpus)\n",
    "    for category in categories:\n",
    "        print(\" {}\".format(category), end=\"\")\n",
    "    print(\"\\n>\\t After Spacy processing:\", end=\"\\n>\\t\\t\")\n",
    "    categories = get_categories_spacy(docs)\n",
    "    for category in categories:\n",
    "        print(\" {}\".format(category), end=\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "28bd3dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Q3.1.1]\n",
      ">\tProviding list of categories in the dataset:\n",
      ">\t Original dataset:\n",
      ">\t\t PER MISC LOC ORG\n",
      ">\t After Spacy processing:\n",
      ">\t\t QUANTITY LOC DATE EVENT PERSON LAW WORK_OF_ART PRODUCT ORDINAL CARDINAL NORP ORG GPE FAC PERCENT MONEY LANGUAGE TIME\n"
     ]
    }
   ],
   "source": [
    "q311(corpus_test, docs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ee198b",
   "metadata": {},
   "source": [
    "#### Q 3.1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "aff543d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "def map_spacy_ents(old_spacy_ner):\n",
    "    allowed = [\"LOC\", \"ORG\", \"O\"]\n",
    "    mapping_dict = {\n",
    "        \"GPE\": \"LOC\",\n",
    "        \"PERSON\": \"PER\",\n",
    "        \"EVENT\": \"MISC\",\n",
    "        \"NORP\": \"MISC\",\n",
    "    }\n",
    "    spacy_ner = list()\n",
    "    for sent in old_spacy_ner:\n",
    "        sent_temp = list()\n",
    "        for token in sent:\n",
    "            is_otag = \"-\" not in token[1]\n",
    "            ent_label = token[1].split(\"-\")[1] if not is_otag else token[1]\n",
    "            if ent_label not in allowed:\n",
    "                if ent_label in mapping_dict.keys():\n",
    "                    ent_label = mapping_dict[ent_label]\n",
    "                else:\n",
    "                    ent_label = \"O\"\n",
    "                    is_otag = True\n",
    "            if is_otag:\n",
    "                sent_temp.append((token[0], ent_label))\n",
    "            else:\n",
    "                sent_temp.append((token[0], f\"{token[1][:2]}{ent_label}\"))\n",
    "        spacy_ner.append(sent_temp)\n",
    "    return spacy_ner\n",
    "\n",
    "def convert_to_sk_cl_report(tuples_list):\n",
    "    return [t[1] for t in tuples_list if t != []]\n",
    "\n",
    "def q312(docs, corpus):\n",
    "    WORD, _, NE = range(3)\n",
    "    gt = list()\n",
    "    for s in corpus.iob_sents():\n",
    "        gt.append([(w[WORD], w[NE]) for w in s])\n",
    "    spacy_ner = list()\n",
    "    for doc in docs:\n",
    "        current_sent = list()\n",
    "        for token in doc:\n",
    "            ent_type = token.ent_iob_\n",
    "            if token.ent_type_ != '':\n",
    "                ent_type += f\"-{token.ent_type_}\"\n",
    "            current_sent.append((token.text, ent_type))\n",
    "        spacy_ner.append(current_sent)\n",
    "\n",
    "\n",
    "    print(\"[Q3.1.2]\\n>\\tProviding overall accuracy:\", end=\"\\n>\\n\")\n",
    "    results_raw = classification_report(\n",
    "        convert_to_sk_cl_report(chain.from_iterable(gt)), \n",
    "        convert_to_sk_cl_report(chain.from_iterable(spacy_ner)),\n",
    "        zero_division=1, output_dict=True)\n",
    "    print(\">\\t Raw results (no mapping): \\n>\\t\\t%.4f\" % results_raw[\"accuracy\"])\n",
    "    # results = evaluate(gt, spacy_ner)\n",
    "    # pd_tbl = pd.DataFrame().from_dict(results, orient='index')\n",
    "    # pd_tbl.round(decimals=3)\n",
    "    # print(pd_tbl)\n",
    "\n",
    "    results_mapping = classification_report(\n",
    "        convert_to_sk_cl_report(chain.from_iterable(gt)), \n",
    "        convert_to_sk_cl_report(chain.from_iterable(map_spacy_ents(spacy_ner))),\n",
    "        output_dict=True)\n",
    "    print(\">\\t Mapping spacy entity labels to grount truth: \\n>\\t\\t%.4f\" % results_mapping[\"accuracy\"])\n",
    "    # # results = evaluate(gt, map_spacy_ents(spacy_ner))\n",
    "    # # pd_tbl = pd.DataFrame().from_dict(results, orient='index')\n",
    "    # # pd_tbl.round(decimals=3)\n",
    "    # # print(pd_tbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "b8a99874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Q3.1.2]\n",
      ">\tProviding overall accuracy:\n",
      ">\n",
      ">\t Raw results (no mapping): \n",
      ">\t\t0.7114\n",
      ">\t Mapping spacy entity labels to grount truth: \n",
      ">\t\t0.9061\n"
     ]
    }
   ],
   "source": [
    "q312(docs_test, corpus_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c9e7f8",
   "metadata": {},
   "source": [
    "#### Q 3.1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2872b158",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q313(docs):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b559fb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "q313(docs_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226b7f98",
   "metadata": {},
   "source": [
    "## Task 4: BONUS PART (extra mark for laude)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64e4b6f",
   "metadata": {},
   "source": [
    "### Save old parser configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b346e653",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.parse.transitionparser import Configuration, TransitionParser\n",
    "old_extract_features = Configuration.extract_features\n",
    "old_train = TransitionParser.train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02c5951",
   "metadata": {},
   "source": [
    "### Retrieve treebank dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e038995f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package dependency_treebank to\n",
      "[nltk_data]     /home/pips/nltk_data...\n",
      "[nltk_data]   Package dependency_treebank is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import types\n",
    "from nltk import download\n",
    "download('dependency_treebank')\n",
    "from nltk.corpus import dependency_treebank\n",
    "from nltk.parse import DependencyEvaluator\n",
    "\n",
    "# split the dataset into train and test\n",
    "# first 100 as train dataset and last 10 as test dataset\n",
    "train_dataset = dependency_treebank.parsed_sents()[:100]\n",
    "test_dataset =  dependency_treebank.parsed_sents()[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b3cfbe",
   "metadata": {},
   "source": [
    "### Compute performance of the original parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d6848910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Number of training examples : 100\n",
      " Number of valid (projective) examples : 100\n",
      "original labelled attachment score  0.7791666666666667\n",
      "original unlabelled attachment score 0.7791666666666667\n"
     ]
    }
   ],
   "source": [
    "Configuration.extract_features = old_extract_features\n",
    "tp = TransitionParser('arc-standard')\n",
    "tp.train(train_dataset, 'tp.model', verbose=False)\n",
    "\n",
    "# parsing takes a list of dependency graphs and a model as arguments\n",
    "parses = tp.parse(test_dataset, 'tp.model')\n",
    "\n",
    "# evaluating the parser\n",
    "de = DependencyEvaluator(parses, test_dataset)\n",
    "las, uas = de.eval()\n",
    "\n",
    "# no labels, thus identical\n",
    "print(\"original labelled attachment score \",las)\n",
    "print(\"original unlabelled attachment score\",uas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7080256",
   "metadata": {},
   "source": [
    "### Q 4.1\n",
    "- Modify NLTK Transition parser's Configuration calss to use better features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1a75ea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['STK_0_POS_TOP',\n",
       " 'STK_0_CTAG_TOP',\n",
       " 'BUF_0_FORM_Economic',\n",
       " 'BUF_0_LEMMA_Economic',\n",
       " 'BUF_0_POS_JJ',\n",
       " 'BUF_0_CTAG_JJ',\n",
       " 'BUF_0_HEAD_2',\n",
       " 'BUF_0_REL_ATT',\n",
       " 'BUF_1_LEMMA_news',\n",
       " 'BUF_1_POS_NN',\n",
       " 'BUF_2_POS_VBD',\n",
       " 'BUF_3_POS_JJ',\n",
       " 'BUF_4_POS_NN']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.parse import DependencyGraph\n",
    "def new_extract_features(self):\n",
    "    \"\"\"\n",
    "    Extract the set of features for the current configuration. Implement standard features as describe in\n",
    "    Table 3.2 (page 31) in Dependency Parsing book by Sandra Kubler, Ryan McDonal, Joakim Nivre.\n",
    "    Please note that these features are very basic.\n",
    "    :return: list(str)\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    # print(\"n\", end=\" \")\n",
    "    if len(self.stack) > 0:\n",
    "        # Stack 0\n",
    "        stack_idx0 = self.stack[len(self.stack) - 1]\n",
    "        token = self._tokens[stack_idx0]\n",
    "        # print (token)\n",
    "        if self._check_informative(token[\"word\"], True):\n",
    "            result.append(\"STK_0_FORM_\" + token[\"word\"])\n",
    "        if \"lemma\" in token and self._check_informative(token[\"lemma\"]):\n",
    "            result.append(\"STK_0_LEMMA_\" + token[\"lemma\"])\n",
    "        if self._check_informative(token[\"tag\"]):\n",
    "            result.append(\"STK_0_POS_\" + token[\"tag\"])\n",
    "        if self._check_informative(token[\"ctag\"], True):\n",
    "            result.append(\"STK_0_CTAG_\" + token[\"ctag\"])\n",
    "        if self._check_informative(token[\"head\"], True):\n",
    "            result.append(\"STK_0_HEAD_\" + str(token[\"head\"]))\n",
    "        if self._check_informative(token[\"rel\"]):\n",
    "            result.append(\"BUF_0_REL_\" + token[\"rel\"])\n",
    "        if \"feats\" in token and self._check_informative(token[\"feats\"]):\n",
    "            feats = token[\"feats\"].split(\"|\")\n",
    "            for feat in feats:\n",
    "                result.append(\"STK_0_FEATS_\" + feat)\n",
    "        # Stack 1\n",
    "        if len(self.stack) > 1:\n",
    "            stack_idx1 = self.stack[len(self.stack) - 2]\n",
    "            token = self._tokens[stack_idx1]\n",
    "            if self._check_informative(token[\"tag\"]):\n",
    "                result.append(\"STK_1_POS_\" + token[\"tag\"])\n",
    "\n",
    "        # Left most, right most dependency of stack[0]\n",
    "        left_most = 1000000\n",
    "        right_most = -1\n",
    "        dep_left_most = \"\"\n",
    "        dep_right_most = \"\"\n",
    "        for (wi, r, wj) in self.arcs:\n",
    "            if wi == stack_idx0:\n",
    "                if (wj > wi) and (wj > right_most):\n",
    "                    right_most = wj\n",
    "                    dep_right_most = r\n",
    "                if (wj < wi) and (wj < left_most):\n",
    "                    left_most = wj\n",
    "                    dep_left_most = r\n",
    "        if self._check_informative(dep_left_most):\n",
    "            result.append(\"STK_0_LDEP_\" + dep_left_most)\n",
    "        if self._check_informative(dep_right_most):\n",
    "            result.append(\"STK_0_RDEP_\" + dep_right_most)\n",
    "\n",
    "    # Check Buffered 0\n",
    "    if len(self.buffer) > 0:\n",
    "        # Buffer 0\n",
    "        buffer_idx0 = self.buffer[0]\n",
    "        token = self._tokens[buffer_idx0]\n",
    "        if self._check_informative(token[\"word\"], True):\n",
    "            result.append(\"BUF_0_FORM_\" + token[\"word\"])\n",
    "        if \"lemma\" in token and self._check_informative(token[\"lemma\"]):\n",
    "            result.append(\"BUF_0_LEMMA_\" + token[\"lemma\"])\n",
    "        if self._check_informative(token[\"tag\"]):\n",
    "            result.append(\"BUF_0_POS_\" + token[\"tag\"])\n",
    "        if self._check_informative(token[\"ctag\"]):\n",
    "            result.append(\"BUF_0_CTAG_\" + token[\"ctag\"])\n",
    "        if self._check_informative(token[\"head\"]):\n",
    "            result.append(\"BUF_0_HEAD_\" + str(token[\"head\"]))\n",
    "        if self._check_informative(token[\"rel\"]):\n",
    "            result.append(\"BUF_0_REL_\" + token[\"rel\"])\n",
    "        if \"feats\" in token and self._check_informative(token[\"feats\"]):\n",
    "            feats = token[\"feats\"].split(\"|\")\n",
    "            for feat in feats:\n",
    "                result.append(\"BUF_0_FEATS_\" + feat)\n",
    "        # Buffer 1\n",
    "        if len(self.buffer) > 1:\n",
    "            buffer_idx1 = self.buffer[1]\n",
    "            token = self._tokens[buffer_idx1]\n",
    "            ### DISCARDED\n",
    "            # if self._check_informative(token[\"word\"], True):\n",
    "            #     result.append(\"BUF_1_FORM_\" + token[\"word\"])\n",
    "            # if self._check_informative(token[\"head\"]):\n",
    "            #     result.append(\"BUF_1_HEAD_\" + str(token[\"head\"]))\n",
    "            if self._check_informative(token[\"lemma\"], True):\n",
    "                result.append(\"BUF_1_LEMMA_\" + token[\"lemma\"])\n",
    "            if self._check_informative(token[\"tag\"]):\n",
    "                result.append(\"BUF_1_POS_\" + token[\"tag\"])\n",
    "        if len(self.buffer) > 2:\n",
    "            buffer_idx2 = self.buffer[2]\n",
    "            token = self._tokens[buffer_idx2]\n",
    "            if self._check_informative(token[\"tag\"]):\n",
    "                result.append(\"BUF_2_POS_\" + token[\"tag\"])\n",
    "        if len(self.buffer) > 3:\n",
    "            buffer_idx3 = self.buffer[3]\n",
    "            token = self._tokens[buffer_idx3]\n",
    "            if self._check_informative(token[\"tag\"]):\n",
    "                result.append(\"BUF_3_POS_\" + token[\"tag\"])\n",
    "        if len(self.buffer) > 4:\n",
    "            buffer_idx4 = self.buffer[4]\n",
    "            token = self._tokens[buffer_idx4]\n",
    "            if self._check_informative(token[\"tag\"]):\n",
    "                result.append(\"BUF_4_POS_\" + token[\"tag\"])\n",
    "                # Left most, right most dependency of stack[0]\n",
    "        left_most = 1000000\n",
    "        right_most = -1\n",
    "        dep_left_most = \"\"\n",
    "        dep_right_most = \"\"\n",
    "        for (wi, r, wj) in self.arcs:\n",
    "            if wi == buffer_idx0:\n",
    "                if (wj > wi) and (wj > right_most):\n",
    "                    right_most = wj\n",
    "                    dep_right_most = r\n",
    "                if (wj < wi) and (wj < left_most):\n",
    "                    left_most = wj\n",
    "                    dep_left_most = r\n",
    "        if self._check_informative(dep_left_most):\n",
    "            result.append(\"BUF_0_LDEP_\" + dep_left_most)\n",
    "        if self._check_informative(dep_right_most):\n",
    "            result.append(\"BUF_0_RDEP_\" + dep_right_most)\n",
    "\n",
    "    return result\n",
    "\n",
    "Configuration.extract_features = new_extract_features\n",
    "\n",
    "\n",
    "gold_sent = DependencyGraph(\"\"\"\n",
    "Economic  JJ     2      ATT\n",
    "news  NN     3       SBJ\n",
    "has       VBD       0       ROOT\n",
    "little      JJ      5       ATT\n",
    "effect   NN     3       OBJ\n",
    "on     IN      5       ATT\n",
    "financial       JJ       8       ATT\n",
    "markets    NNS      6       PC\n",
    ".    .      3       PU\n",
    "\"\"\")\n",
    "\n",
    "# for s in gold_sent.triples():\n",
    "#     print(s)\n",
    "\n",
    "conf = Configuration(gold_sent)\n",
    "# print(conf)\n",
    "conf.extract_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406bd578",
   "metadata": {},
   "source": [
    "### Q 4.2\n",
    "- Evaluate the features comparing performance to the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe9c9951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Number of training examples : 100\n",
      " Number of valid (projective) examples : 100\n",
      "modified labelled attachment score 0.8\n",
      "modified unlabelled attachment score 0.8\n"
     ]
    }
   ],
   "source": [
    "Configuration.extract_features = new_extract_features\n",
    "# using the TransitionParser \n",
    "tp_new_fe = TransitionParser('arc-standard')\n",
    "# replacing the train function with the modified one\n",
    "# tp_new_fe = types.MethodType(train,tp_new_fe)\n",
    "tp_new_fe.train(train_dataset, 'tp_new_fe.model', verbose=False)\n",
    "# print(tp)\n",
    "\n",
    "# parsing takes a list of dependency graphs and a model as arguments\n",
    "parses_new_fe = tp_new_fe.parse(test_dataset, 'tp_new_fe.model')\n",
    "# print(len(parses_new_fe))\n",
    "# print(parses_new_fe[0])\n",
    "\n",
    "de_new_fe = DependencyEvaluator(parses_new_fe, test_dataset)\n",
    "las_new_fe, uas_new_fe = de_new_fe.eval()\n",
    "\n",
    "# no labels, thus identical\n",
    "print('modified labelled attachment score',las_new_fe)\n",
    "print('modified unlabelled attachment score',uas_new_fe)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1408d421",
   "metadata": {},
   "source": [
    "### Q 4.3\n",
    "- Replace SVM classifier with an alternative of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6ab09524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LINEAR CLASSIFIER\n",
      "\n",
      " Number of training examples : 100\n",
      " Number of valid (projective) examples : 100\n",
      "modified classifier labelled attachment score 0.6833333333333333\n",
      "modified classifier unlabelled attachment score 0.6833333333333333\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import tempfile\n",
    "try:\n",
    "    from sklearn.linear_model import SGDClassifier\n",
    "    from sklearn.datasets import load_svmlight_file\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "def sgd_train(self, depgraphs, modelfile, verbose=True):\n",
    "    \"\"\"\n",
    "    :param depgraphs : list of DependencyGraph as the training data\n",
    "    :type depgraphs : DependencyGraph\n",
    "    :param modelfile : file name to save the trained model\n",
    "    :type modelfile : str\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        input_file = tempfile.NamedTemporaryFile(\n",
    "            prefix=\"transition_parse.train\", dir=tempfile.gettempdir(), delete=False\n",
    "        )\n",
    "\n",
    "        if self._algorithm == self.ARC_STANDARD:\n",
    "            self._create_training_examples_arc_std(depgraphs, input_file)\n",
    "        else:\n",
    "            self._create_training_examples_arc_eager(depgraphs, input_file)\n",
    "\n",
    "        input_file.close()\n",
    "        # Using the temporary file to train the libsvm classifier\n",
    "        x_train, y_train = load_svmlight_file(input_file.name)\n",
    "\n",
    "        model = SGDClassifier(\n",
    "            loss=\"log\",\n",
    "            penalty=\"l2\",\n",
    "            shuffle=True,\n",
    "            verbose=0,\n",
    "            learning_rate=\"optimal\"\n",
    "        )\n",
    "        model.fit(x_train, y_train)\n",
    "        # Save the model to file name (as pickle)\n",
    "        pickle.dump(model, open(modelfile, \"wb\"))\n",
    "    finally:\n",
    "        os.remove(input_file.name)\n",
    "\n",
    "print(\"LINEAR CLASSIFIER\\n\")\n",
    "tp_sgd_clf = TransitionParser('arc-standard')\n",
    "tp_sgd_clf.train = types.MethodType(sgd_train,tp_sgd_clf)\n",
    "tp_sgd_clf.train(train_dataset, 'tp_sgd_clf.model', verbose=False)\n",
    "parses_sgd_clf = tp_sgd_clf.parse(test_dataset, 'tp_sgd_clf.model')\n",
    "de_sgd_clf = DependencyEvaluator(parses_sgd_clf, test_dataset)\n",
    "las_sgd_clf, uas_sgd_clf = de_sgd_clf.eval()\n",
    "print('modified classifier labelled attachment score',las_sgd_clf)\n",
    "print('modified classifier unlabelled attachment score',uas_sgd_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e3f0049a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RANDOM FOREST CLASSIFIER\n",
      "\n",
      " Number of training examples : 100\n",
      " Number of valid (projective) examples : 100\n",
      "modified classifier labelled attachment score 0.7583333333333333\n",
      "modified classifier unlabelled attachment score 0.7583333333333333\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import tempfile\n",
    "try:\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.datasets import load_svmlight_file\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "def rf_train(self, depgraphs, modelfile, verbose=True):\n",
    "    \"\"\"\n",
    "    :param depgraphs : list of DependencyGraph as the training data\n",
    "    :type depgraphs : DependencyGraph\n",
    "    :param modelfile : file name to save the trained model\n",
    "    :type modelfile : str\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        input_file = tempfile.NamedTemporaryFile(\n",
    "            prefix=\"transition_parse.train\", dir=tempfile.gettempdir(), delete=False\n",
    "        )\n",
    "\n",
    "        if self._algorithm == self.ARC_STANDARD:\n",
    "            self._create_training_examples_arc_std(depgraphs, input_file)\n",
    "        else:\n",
    "            self._create_training_examples_arc_eager(depgraphs, input_file)\n",
    "\n",
    "        input_file.close()\n",
    "        # Using the temporary file to train the libsvm classifier\n",
    "        x_train, y_train = load_svmlight_file(input_file.name)\n",
    "\n",
    "        model = RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            criterion=\"entropy\",\n",
    "            max_features=\"log2\"\n",
    "        )\n",
    "        \n",
    "        model.fit(x_train, y_train)\n",
    "        # Save the model to file name (as pickle)\n",
    "        pickle.dump(model, open(modelfile, \"wb\"))\n",
    "    finally:\n",
    "        os.remove(input_file.name)\n",
    "\n",
    "print(\"RANDOM FOREST CLASSIFIER\\n\")\n",
    "tp_rf_clf = TransitionParser('arc-standard')\n",
    "tp_rf_clf.train = types.MethodType(rf_train,tp_rf_clf)\n",
    "tp_rf_clf.train(train_dataset, 'tp_rf_clf.model', verbose=False)\n",
    "parses_rf_clf = tp_rf_clf.parse(test_dataset, 'tp_rf_clf.model')\n",
    "de_rf_clf = DependencyEvaluator(parses_rf_clf, test_dataset)\n",
    "las_rf_clf, uas_rf_clf = de_rf_clf.eval()\n",
    "print('modified classifier labelled attachment score',las_rf_clf)\n",
    "print('modified classifier unlabelled attachment score',uas_rf_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "dea0c90b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP CLASSIFIER\n",
      "\n",
      " Number of training examples : 100\n",
      " Number of valid (projective) examples : 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pips/Projects/NLU-UniTN-2022/venv/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified classifier labelled attachment score 0.7708333333333334\n",
      "modified classifier unlabelled attachment score 0.7708333333333334\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import tempfile\n",
    "try:\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from sklearn.datasets import load_svmlight_file\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "def mlp_train(self, depgraphs, modelfile, verbose=True):\n",
    "    \"\"\"\n",
    "    :param depgraphs : list of DependencyGraph as the training data\n",
    "    :type depgraphs : DependencyGraph\n",
    "    :param modelfile : file name to save the trained model\n",
    "    :type modelfile : str\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        input_file = tempfile.NamedTemporaryFile(\n",
    "            prefix=\"transition_parse.train\", dir=tempfile.gettempdir(), delete=False\n",
    "        )\n",
    "\n",
    "        if self._algorithm == self.ARC_STANDARD:\n",
    "            self._create_training_examples_arc_std(depgraphs, input_file)\n",
    "        else:\n",
    "            self._create_training_examples_arc_eager(depgraphs, input_file)\n",
    "\n",
    "        input_file.close()\n",
    "        # Using the temporary file to train the libsvm classifier\n",
    "        x_train, y_train = load_svmlight_file(input_file.name)\n",
    "        \n",
    "        model = MLPClassifier(\n",
    "            activation=\"relu\", \n",
    "            learning_rate=\"adaptive\",\n",
    "            solver='sgd', \n",
    "            nesterovs_momentum=True,\n",
    "            alpha=1e-5,\n",
    "            hidden_layer_sizes=(20,50,20), \n",
    "            random_state=1\n",
    "            )\n",
    "\n",
    "\n",
    "        model.fit(x_train, y_train)\n",
    "        # Save the model to file name (as pickle)\n",
    "        pickle.dump(model, open(modelfile, \"wb\"))\n",
    "    finally:\n",
    "        os.remove(input_file.name)\n",
    "\n",
    "print(\"MLP CLASSIFIER\\n\")\n",
    "tp_mlp_clf = TransitionParser('arc-standard')\n",
    "tp_mlp_clf.train = types.MethodType(mlp_train,tp_mlp_clf)\n",
    "tp_mlp_clf.train(train_dataset, 'tp_mlp_clf.model', verbose=False)\n",
    "parses_mlp_clf = tp_mlp_clf.parse(test_dataset, 'tp_mlp_clf.model')\n",
    "de_mlp_clf = DependencyEvaluator(parses_mlp_clf, test_dataset)\n",
    "las_mlp_clf, uas_mlp_clf = de_mlp_clf.eval()\n",
    "print('modified classifier labelled attachment score',las_mlp_clf)\n",
    "print('modified classifier unlabelled attachment score',uas_mlp_clf)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b8ea92f07f1cce96500be7f9231af4df2b17add51555df0958a949f03550b4d9"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
