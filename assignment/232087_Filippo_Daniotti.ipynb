{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ac00029",
   "metadata": {},
   "source": [
    "# NLU: Mid-Term Assignment 2022\n",
    "### Description\n",
    "In this notebook, we ask you to complete four main tasks to show what you have learnt during the NLU labs. Therefore, to complete the assignment please refer to the concepts, libraries and other materials shown and used during the labs. The last task is not mandatory, it is a *BONUS* to get an extra mark for the laude. \n",
    "\n",
    "### Instructions\n",
    "- **Dataset**: in this notebook, you are asked to work with the dataset *Conll 2003* provided by us in the *data* folder. Please, load the files from the *data* folder and **do not** change names or paths of the inner files. \n",
    "- **Output**: for each part of your task, print your results and leave it in the notebook. Please, **do not** send a jupyter notebook without the printed outputs.\n",
    "- **Other**: follow carefully all the further instructions and suggestions given in the question descriptions.\n",
    "\n",
    "### Deadline\n",
    "The deadline is due in two weeks from the project presentation. Please, refer to *piazza* channel for the exact date."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e1f620",
   "metadata": {},
   "source": [
    "## Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7322446f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "from nltk.lm import Vocabulary\n",
    "from nltk.corpus import ConllCorpusReader\n",
    "\n",
    "CORPUS_ROOT = 'data'\n",
    "CORPUS_FILEIDS = ['train.txt', 'test.txt', 'valid.txt']\n",
    "CORPUS_COLUMNTYPES = ['words', 'ne', 'pos', 'chunk', 'tree']\n",
    "\n",
    "corpus = ConllCorpusReader(CORPUS_ROOT, CORPUS_FILEIDS, CORPUS_COLUMNTYPES)\n",
    "corpus_train = ConllCorpusReader(CORPUS_ROOT, CORPUS_FILEIDS[0], CORPUS_COLUMNTYPES)\n",
    "corpus_test = ConllCorpusReader(CORPUS_ROOT, CORPUS_FILEIDS[1], CORPUS_COLUMNTYPES)\n",
    "corpus_val = ConllCorpusReader(CORPUS_ROOT, CORPUS_FILEIDS[2], CORPUS_COLUMNTYPES)\n",
    "\n",
    "# Removing DOCSTART and other empty lists\n",
    "corpus_sents = [s for s in corpus.sents() if s != []]\n",
    "corpus_train_sents = [s for s in corpus_train.sents() if s != []]\n",
    "corpus_test_sents = [s for s in corpus_test.sents() if s != []]\n",
    "corpus_val_sents = [s for s in corpus_val.sents() if s != []]\n",
    "\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.tokenizer = Tokenizer(nlp.vocab)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath('./src/'))\n",
    "from conll import evaluate\n",
    "\n",
    "import pandas as pd\n",
    "from itertools import chain\n",
    "\n",
    "# Utilities\n",
    "def nbest(d, n=1):\n",
    "    \"\"\"\n",
    "    get n max values from a dict\n",
    "    :param d: input dict (values are numbers, keys are stings)\n",
    "    :param n: number of values to get (int)\n",
    "    :return: dict of top n key-value pairs\n",
    "    \"\"\"\n",
    "    return dict(sorted(d.items(), key=lambda item: item[1], reverse=True)[:n])\n",
    "\n",
    "def get_flat_sents(corpus):\n",
    "    sents = list()\n",
    "    for sent in corpus:\n",
    "        flat_sent = \" \".join(sent)\n",
    "        sents.append(flat_sent.strip())\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f650afd4",
   "metadata": {},
   "source": [
    "#### Note\n",
    "Questions 1.1 to 2.3 are conveniently wrapped into containing functions `qXX()` (e.g. `q23()` for question 2.3) in order to restrict the scope of names."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f40e1f",
   "metadata": {},
   "source": [
    "## Task 1: Analysis of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18406477",
   "metadata": {},
   "source": [
    "### Q 1.1\n",
    "- Create the Vocabulary and Frequency Dictionary of the:\n",
    "    1. Whole dataset\n",
    "    2. Train set\n",
    "    3. Test set\n",
    "    \n",
    "**Attention**: print the first 20 words of the Dictionaty of each set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "811cdb47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of whole dataset: 26869\n",
      "Length of train set: 21009\n",
      "Length of test set: 8548\n",
      "\n",
      "First 20 words of whole dataset:\n",
      "{'the': 12310, ',': 10876, '.': 10874, 'of': 5502, 'in': 5405, 'to': 5129, 'a': 4731, '(': 4226, ')': 4225, 'and': 4223, '\"': 3239, 'on': 3115, 'said': 2694, \"'s\": 2339, 'for': 2109, '-': 1866, '1': 1845, 'at': 1679, 'was': 1593, '2': 1342}\n",
      "\n",
      "First 20 words of train set:\n",
      "{'the': 8390, '.': 7374, ',': 7290, 'of': 3815, 'in': 3621, 'to': 3424, 'a': 3199, 'and': 2872, '(': 2861, ')': 2861, '\"': 2178, 'on': 2092, 'said': 1849, \"'s\": 1566, 'for': 1465, '1': 1421, '-': 1243, 'at': 1146, 'was': 1095, '2': 973}\n",
      "\n",
      "First 20 words of test set:\n",
      "{'the': 1765, ',': 1637, '.': 1626, 'to': 805, 'of': 789, 'in': 761, '(': 686, ')': 684, 'a': 658, 'and': 598, 'on': 467, '\"': 421, 'said': 399, \"'s\": 347, '-': 287, 'for': 286, 'at': 251, 'was': 224, '4': 201, 'with': 185}\n"
     ]
    }
   ],
   "source": [
    "def q11():\n",
    "    # Create vocabulary\n",
    "    vocab = set([w.lower() for w in corpus.words()])\n",
    "    vocab_train = set([w.lower() for w in corpus_train.words()])\n",
    "    vocab_test = set([w.lower() for w in corpus_test.words()])\n",
    "\n",
    "    # Create frequency distribution\n",
    "    fd = FreqDist([w.lower() for w in corpus.words()])\n",
    "    fd_train = FreqDist([w.lower() for w in corpus_train.words()])\n",
    "    fd_test = FreqDist([w.lower() for w in corpus_test.words()])\n",
    "\n",
    "    # Print vocabulary length\n",
    "    print(\"Length of whole dataset: %d\" % len(vocab))\n",
    "    print(\"Length of train set: %d\" % len(vocab_train))\n",
    "    print(\"Length of test set: %d\" % len(vocab_test))\n",
    "\n",
    "    # Print the first 20 words for each dict\n",
    "    print(\"\\nFirst 20 words of whole dataset:\")\n",
    "    print(nbest(fd, 20))\n",
    "    print(\"\\nFirst 20 words of train set:\")\n",
    "    print(nbest(fd_train, 20))\n",
    "    print(\"\\nFirst 20 words of test set:\")\n",
    "    print(nbest(fd_test, 20))\n",
    "\n",
    "q11()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6e75cf",
   "metadata": {},
   "source": [
    "### Q 1.2\n",
    "- Obtain the list of:\n",
    "    1. Out-Of-Vocabulary (OOV) tokens\n",
    "    2. Overlapping tokens between train and test sets  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01dd3358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Q1.2.1]\n",
      ">\tOOV tokens:\n",
      ">\t (test) Found 3268 OOV\n",
      ">\t (valid) Found 2856 OOV\n",
      ">\t (test + valid) Found 0 OOV\n",
      "\n",
      "[Q1.2.1]\n",
      ">\tOverlapping tokens:\n",
      ">\t (test) Found 5280 overlapping tokens\n",
      ">\t (valid) Found 6146 overlapping tokens\n",
      ">\t (test + val) Found 8066 overlapping tokens\n"
     ]
    }
   ],
   "source": [
    "def q12(cutoff=1):\n",
    "\n",
    "\n",
    "    test_lower = [w.lower() for w in corpus_test.words()]\n",
    "    val_lower = [w.lower() for w in corpus_val.words()]\n",
    "    # Get vocabs\n",
    "    vocab_train = Vocabulary([w.lower() for w in corpus_train.words()], unk_cutoff=cutoff)\n",
    "    vocab_test = Vocabulary(test_lower, unk_cutoff=cutoff)\n",
    "    vocab_valid = Vocabulary(val_lower, unk_cutoff=cutoff)\n",
    "    vocab_tv = Vocabulary([*test_lower, *val_lower], unk_cutoff=cutoff)\n",
    "\n",
    "    # Get list of tokens\n",
    "    tokens_train = set(vocab_train.counts.keys())\n",
    "    tokens_test = set(vocab_test.counts.keys())\n",
    "    tokens_val = set(vocab_valid.counts.keys())\n",
    "    tokens_tv = set(vocab_tv.counts.keys())\n",
    "\n",
    "    # Get OOV \n",
    "    oov_test = tokens_test.difference(tokens_train)\n",
    "    oov_valid = tokens_val.difference(tokens_train)\n",
    "    oov_tv = tokens_val.difference(tokens_tv)\n",
    "    print(\"[Q1.2.1]\\n>\\tOOV tokens:\")\n",
    "    print(\">\\t (test) Found {} OOV\".format(len(oov_test)))\n",
    "    print(\">\\t (valid) Found {} OOV\".format(len(oov_valid)))\n",
    "    print(\">\\t (test + valid) Found {} OOV\".format(len(oov_tv)))\n",
    "\n",
    "    print()\n",
    "\n",
    "    # Get overlapping tokens w/ test set\n",
    "    intersection_test = tokens_train.intersection(tokens_test)\n",
    "    intersection_val = tokens_train.intersection(tokens_val)\n",
    "    intersection_tv = tokens_train.intersection(tokens_tv)\n",
    "    print(\"[Q1.2.1]\\n>\\tOverlapping tokens:\")\n",
    "    print(\">\\t (test) Found {} overlapping tokens\".format(len(intersection_test)))\n",
    "    print(\">\\t (valid) Found {} overlapping tokens\".format(len(intersection_val)))\n",
    "    print(\">\\t (test + val) Found {} overlapping tokens\".format(len(intersection_tv)))\n",
    "    \n",
    "\n",
    "q12()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0f8734",
   "metadata": {},
   "source": [
    "### Q 1.3\n",
    "- Perform a complete data analysis of the whole dataset (train + test sets) to obtain:\n",
    "    1. Average sentence length computed in number of tokens\n",
    "    2. The 50 most-common tokens\n",
    "    3. Number of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b7213d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Q1.3.1]\n",
      ">\tAverage sentence length in tokens: 14.5304\n",
      "\n",
      "[Q1.3.2]\n",
      ">\t50 most common tokens:\n",
      ">\t[1] the: 12310\n",
      ">\t[2] ,: 10876\n",
      ">\t[3] .: 10874\n",
      ">\t[4] of: 5502\n",
      ">\t[5] in: 5405\n",
      ">\t[6] to: 5129\n",
      ">\t[7] a: 4731\n",
      ">\t[8] (: 4226\n",
      ">\t[9] ): 4225\n",
      ">\t[10] and: 4223\n",
      ">\t[11] \": 3239\n",
      ">\t[12] on: 3115\n",
      ">\t[13] said: 2694\n",
      ">\t[14] 's: 2339\n",
      ">\t[15] for: 2109\n",
      ">\t[16] -: 1866\n",
      ">\t[17] 1: 1845\n",
      ">\t[18] at: 1679\n",
      ">\t[19] was: 1593\n",
      ">\t[20] 2: 1342\n",
      ">\t[21] with: 1267\n",
      ">\t[22] 3: 1264\n",
      ">\t[23] 0: 1232\n",
      ">\t[24] that: 1212\n",
      ">\t[25] he: 1166\n",
      ">\t[26] from: 1146\n",
      ">\t[27] by: 1113\n",
      ">\t[28] it: 1082\n",
      ">\t[29] :: 1057\n",
      ">\t[30] is: 984\n",
      ">\t[31] 4: 973\n",
      ">\t[32] as: 920\n",
      ">\t[33] his: 867\n",
      ">\t[34] had: 841\n",
      ">\t[35] were: 804\n",
      ">\t[36] an: 796\n",
      ">\t[37] but: 786\n",
      ">\t[38] not: 786\n",
      ">\t[39] after: 780\n",
      ">\t[40] has: 768\n",
      ">\t[41] be: 754\n",
      ">\t[42] have: 738\n",
      ">\t[43] new: 656\n",
      ">\t[44] first: 645\n",
      ">\t[45] who: 643\n",
      ">\t[46] 5: 636\n",
      ">\t[47] will: 591\n",
      ">\t[48] 6: 584\n",
      ">\t[49] two: 579\n",
      ">\t[50] they: 567\n",
      "\n",
      "[Q1.3.3]\n",
      ">\tNumber of sentences: 20744\n"
     ]
    }
   ],
   "source": [
    "def q13():\n",
    "\n",
    "    # Get average sentence length\n",
    "    print(\"[Q1.3.1]\\n>\\tAverage sentence length in tokens: {:.4f}\\n\".format(len(corpus.words())/len(corpus_sents)))\n",
    "\n",
    "    # Get 50 most common tokens\n",
    "    vocab = Vocabulary([w.lower() for w in corpus.words()])\n",
    "    most_common_tokens = nbest(vocab.counts, 50)\n",
    "    print(\"[Q1.3.2]\\n>\\t50 most common tokens:\")\n",
    "    # print(\">\\t\", most_common_tokens)\n",
    "    count = 1\n",
    "    for key in most_common_tokens:\n",
    "        print(\">\\t[{}] {}: {}\".format(count, key, most_common_tokens[key]))\n",
    "        count += 1\n",
    "\n",
    "    # Get number of sentences\n",
    "    print(\"\\n[Q1.3.3]\\n>\\tNumber of sentences: %d\" % len(corpus_sents))\n",
    "\n",
    "q13()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9fb7a7",
   "metadata": {},
   "source": [
    "### Q 1.4\n",
    "- Create the dictionary of Named Entities and their Frequencies for the:\n",
    "    1. Whole dataset\n",
    "    2. Train set\n",
    "    3. Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f35a5b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Q1.4.1]\n",
      ">\tFrequency dist of Named Entities for the whole dataset\n",
      ">\t {'U.S.': 460, 'Germany': 237, 'Australia': 204, 'France': 199, 'England': 176, 'Russia': 167, 'Britain': 165, 'Italy': 160, 'China': 149, 'LONDON': 147, 'Spain': 145, 'NEW YORK': 143, 'Japan': 133, 'Russian': 120, 'German': 114, 'Reuters': 114, 'Israel': 108, 'Sweden': 108, 'Pakistan': 103, 'Iraq': 98}\n",
      "[Q1.4.2]\n",
      ">\tFrequency dist of Named Entities for the training set\n",
      ">\t {'U.S.': 303, 'Germany': 141, 'Britain': 133, 'Australia': 130, 'England': 123, 'France': 122, 'Spain': 110, 'Italy': 98, 'NEW YORK': 95, 'LONDON': 93, 'Russian': 92, 'China': 91, 'Russia': 88, 'Japan': 87, 'Pakistan': 85, 'Sweden': 81, 'German': 80, 'British': 73, 'Reuters': 73, 'Belgium': 71}\n",
      "[Q1.4.3]\n",
      ">\tFrequency dist of Named Entities for the test set\n",
      ">\t {'Germany': 49, 'U.S.': 45, 'Australia': 45, 'Japan': 41, 'Italy': 41, 'France': 40, 'World Cup': 34, 'Russia': 34, 'Indonesia': 33, 'China': 32, 'LONDON': 31, 'Austria': 29, 'Barcelona': 24, 'Canada': 23, 'NEW YORK': 22, 'England': 21, 'Singapore': 21, 'Poland': 20, 'Reuters': 19, 'WTO': 19}\n"
     ]
    }
   ],
   "source": [
    "def q14():\n",
    "    WORD, _, NE = range(3)\n",
    "\n",
    "    def merge_iob_tags(doc):\n",
    "        idx = 0\n",
    "        merged_ne = list()\n",
    "        for idx in range(len(doc)):\n",
    "            if doc[idx][NE].split('-')[0] == \"B\":\n",
    "                temp = str(doc[idx][WORD])\n",
    "                idx += 1\n",
    "                while idx < len(doc) and doc[idx][NE].split('-')[0] == \"I\":\n",
    "                    temp += \" %s\" % str(doc[idx][WORD])\n",
    "                    idx += 1\n",
    "                merged_ne.append(temp)\n",
    "        return merged_ne\n",
    "\n",
    "    # Whole dataset\n",
    "    iob_all = [(w[WORD], _, w[NE]) for w in corpus.iob_words() if w[NE] != 'O']\n",
    "    ne_all = merge_iob_tags(iob_all)\n",
    "    fd_all = FreqDist(ne_all)\n",
    "    print(\"[Q1.4.1]\\n>\\tFrequency dist of Named Entities for the whole dataset\\n>\\t\", nbest(fd_all, 20))\n",
    "\n",
    "    # Train set\n",
    "    iob_train = [(w[WORD], _, w[NE]) for w in corpus_train.iob_words() if w[NE] != 'O']\n",
    "    ne_train = merge_iob_tags(iob_train)\n",
    "    fd_train = FreqDist(ne_train)\n",
    "    print(\"[Q1.4.2]\\n>\\tFrequency dist of Named Entities for the training set\\n>\\t\", nbest(fd_train, 20))\n",
    "\n",
    "    # Test set\n",
    "    iob_test = [(w[WORD], _, w[NE]) for w in corpus_test.iob_words() if w[NE] != 'O']\n",
    "    ne_test = merge_iob_tags(iob_test)\n",
    "    fd_test = FreqDist(ne_test)\n",
    "    print(\"[Q1.4.3]\\n>\\tFrequency dist of Named Entities for the test set\\n>\\t\", nbest(fd_test, 20))\n",
    "\n",
    "q14()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed42c93",
   "metadata": {},
   "source": [
    "## Task 2: Working with Dependecy Tree\n",
    "*Suggestions: use Spacy pipeline to retreive the Dependecy Tree*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76d1903",
   "metadata": {},
   "source": [
    "### Q 2.1\n",
    "- Given each sentence in the dataset, write the required functions to provide:\n",
    "    1. Subject, obects (direct and indirect)\n",
    "    2. Noun chunks\n",
    "    3. The head noun in each noun chunk\n",
    "    \n",
    "**Attention**: *print only the results of these functions by using the sentence \"I saw the man with a telescope\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0b3b578e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I saw the man with a telescope \n",
      "\n",
      "[Q2.1.1]\n",
      ">\tProviding subjects and objects:\n",
      ">\t nsubj: ['I']\n",
      ">\t csubj: []\n",
      ">\t dobj: ['man']\n",
      ">\t pobj: ['telescope']\n",
      ">\t dative: []\n",
      "\n",
      "[Q2.1.2]\n",
      ">\tProviding noun chunks:\n",
      ">\t I\n",
      ">\t the man\n",
      ">\t a telescope\n",
      "\n",
      "[Q2.1.3]\n",
      ">\tProviding head noun for each noun chunk:\n",
      ">\t'CHUNK' -> HEAD\n",
      ">\n",
      ">\t'I' -> I\n",
      ">\t'the man' -> man\n",
      ">\t'a telescope' -> telescope\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def q21(corpus):\n",
    "    \"\"\"\n",
    "    Wrapper function\n",
    "    Input should be a list of lists of token, e.g.:\n",
    "    [\n",
    "        [\"I\", \"saw\", \"the\", \"man\", \"with\", \"a\", \"telescope\"]\n",
    "    ]\n",
    "    \"\"\"\n",
    "\n",
    "    def get_subj_obj_dict(doc):\n",
    "        \"\"\"\n",
    "        Returns the dictionary of dependencies in a spacy-processed\n",
    "        sentence\n",
    "        \"\"\"\n",
    "        deps_dict = dict()\n",
    "        deps = [\n",
    "            'nsubj',  # nominal subject\n",
    "            'csubj',  # clausal subject\n",
    "            'dobj',   # direct object\n",
    "            'pobj',   # object of a preposition (indirect)\n",
    "            'dative'  # object of dative complement\n",
    "        ]\n",
    "        for dep in deps:\n",
    "            deps_dict[dep] = list()\n",
    "        for token in doc:\n",
    "            if token.dep_ in deps:\n",
    "                deps_dict[token.dep_].append(token.text)\n",
    "        return deps_dict\n",
    "                    \n",
    "    def get_noun_chunks(doc):\n",
    "        return doc.noun_chunks\n",
    "\n",
    "    def get_head_of_chunk(doc):\n",
    "        \"\"\"\n",
    "        Returns a list for all chunks in a spacy-processed sentence\n",
    "        of tuples (head_of_chunk, chunk_text)\n",
    "        \"\"\"\n",
    "        return [(c.root.text, c.text) for c in doc.noun_chunks]\n",
    "\n",
    "\n",
    "    def q211(doc):\n",
    "        print(\"[Q2.1.1]\\n>\\tProviding subjects and objects:\")\n",
    "        deps_dict = get_subj_obj_dict(doc)\n",
    "        for key in deps_dict:\n",
    "            print(\">\\t {}: {}\".format(key, deps_dict[key]))\n",
    "        print()\n",
    "\n",
    "\n",
    "    def q212(doc):\n",
    "        print(\"[Q2.1.2]\\n>\\tProviding noun chunks:\")\n",
    "        noun_chunks = get_noun_chunks(doc)\n",
    "        for chunk in noun_chunks:\n",
    "            print(\">\\t\", chunk)\n",
    "        print()\n",
    "\n",
    "    def q213(doc):\n",
    "        print(\"[Q2.1.3]\\n>\\tProviding head noun for each noun chunk:\")\n",
    "        print(\">\\t'CHUNK' -> HEAD\\n>\")\n",
    "        heads = get_head_of_chunk(doc)\n",
    "        for head, chunk in heads:\n",
    "            print(\">\\t'{}' -> {}\".format(chunk, head))\n",
    "        print()\n",
    "\n",
    "\n",
    "    sents = get_flat_sents(corpus)\n",
    "    for sent in sents[:10]:\n",
    "        doc = nlp(sent)\n",
    "        print (sent, \"\\n\")\n",
    "        q211(doc)\n",
    "        q212(doc)\n",
    "        q213(doc)\n",
    "\n",
    "q21([\"I saw the man with a telescope\".split(\" \")])\n",
    "# q21(corpus_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a103b89e",
   "metadata": {},
   "source": [
    "### Q 2.2\n",
    "- Given a dependecy tree of a sentence and a segment of that sentence write the required functions that ouput the dependency subtree of that segment.\n",
    "\n",
    "**Attention**: *print only the results of these functions by using the sentence \"I saw the man with a telescope\" (the segment could be any e.g. \"saw the man\", \"a telescope\", etc.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8ae3c686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I saw the man with the telescope\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"08641c5c9af044bf84860577c74be650-0\" class=\"displacy\" width=\"1275\" height=\"312.0\" direction=\"ltr\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">I</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">saw</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">man</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">with</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">telescope</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-08641c5c9af044bf84860577c74be650-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,89.5 220.0,89.5 220.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-08641c5c9af044bf84860577c74be650-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-08641c5c9af044bf84860577c74be650-0-1\" stroke-width=\"2px\" d=\"M420,177.0 C420,89.5 570.0,89.5 570.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-08641c5c9af044bf84860577c74be650-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,179.0 L412,167.0 428,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-08641c5c9af044bf84860577c74be650-0-2\" stroke-width=\"2px\" d=\"M245,177.0 C245,2.0 575.0,2.0 575.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-08641c5c9af044bf84860577c74be650-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M575.0,179.0 L583.0,167.0 567.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-08641c5c9af044bf84860577c74be650-0-3\" stroke-width=\"2px\" d=\"M595,177.0 C595,89.5 745.0,89.5 745.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-08641c5c9af044bf84860577c74be650-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M745.0,179.0 L753.0,167.0 737.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-08641c5c9af044bf84860577c74be650-0-4\" stroke-width=\"2px\" d=\"M945,177.0 C945,89.5 1095.0,89.5 1095.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-08641c5c9af044bf84860577c74be650-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M945,179.0 L937,167.0 953,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-08641c5c9af044bf84860577c74be650-0-5\" stroke-width=\"2px\" d=\"M770,177.0 C770,2.0 1100.0,2.0 1100.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-08641c5c9af044bf84860577c74be650-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1100.0,179.0 L1108.0,167.0 1092.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk: I\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"2ae5a07d606a47c69f4bbd7e5b736480-0\" class=\"displacy\" width=\"225\" height=\"137.0\" direction=\"ltr\" style=\"max-width: none; height: 137.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"47.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">I</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
       "</text>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk: the man\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"8d49d71351eb485ebc12d856967d1fb8-0\" class=\"displacy\" width=\"400\" height=\"224.5\" direction=\"ltr\" style=\"max-width: none; height: 224.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">man</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-8d49d71351eb485ebc12d856967d1fb8-0-0\" stroke-width=\"2px\" d=\"M70,89.5 C70,2.0 225.0,2.0 225.0,89.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-8d49d71351eb485ebc12d856967d1fb8-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,91.5 L62,79.5 78,79.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk: the telescope\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"3f68ac8bf2e240ea87d86b6fef044820-0\" class=\"displacy\" width=\"400\" height=\"224.5\" direction=\"ltr\" style=\"max-width: none; height: 224.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">telescope</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3f68ac8bf2e240ea87d86b6fef044820-0-0\" stroke-width=\"2px\" d=\"M70,89.5 C70,2.0 225.0,2.0 225.0,89.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3f68ac8bf2e240ea87d86b6fef044820-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,91.5 L62,79.5 78,79.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO\n",
    "# a string representing a chunk is provided\n",
    "# look for the chunk in the noun_chunk\n",
    "# return the span holding the subtree\n",
    "\n",
    "def q22(corpus):\n",
    "    \"\"\"\n",
    "    Wrapper function\n",
    "    Input should be a list of lists of token, e.g.:\n",
    "    [\n",
    "        [\"I\", \"saw\", \"the\", \"man\", \"with\", \"a\", \"telescope\"]\n",
    "    ]\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def get_subtree(chunk_text, doc):\n",
    "        subtree = None\n",
    "        chunk_doc = nlp(chunk_text)\n",
    "        # chunk_root = get_root(chunk_doc)\n",
    "        # for token in doc:\n",
    "        #     if token.text == chunk_root:\n",
    "        #         leftmost = list(token.subtree)[0].i\n",
    "        #         rightmost = list(token.subtree)[-1].i\n",
    "        #         subtree = doc[leftmost:rightmost+1]\n",
    "        # return subtree\n",
    "    \n",
    "    # test segments\n",
    "    # NB: it is assumed that all provided segments are valid\n",
    "    tests = [\"the man\", \"the telescope\"]\n",
    "    sents = get_flat_sents(corpus)\n",
    "    # sents = [sents[14]]\n",
    "    for sent in sents:\n",
    "        print(sent)\n",
    "        doc = nlp(sent)\n",
    "        spacy.displacy.render(doc, style=\"dep\")\n",
    "        for test in tests:\n",
    "            subtree = get_subtree(test, doc)\n",
    "            # print(\"Chunk: {}\\n>\\tSubtree: {}\".format(chunk, chunk))\n",
    "            # print(\"Chunk: {}\".format(chunk))\n",
    "            # spacy.displacy.render(chunk, style=\"dep\")\n",
    "    \n",
    "q22([\n",
    "    \"I saw the man with the telescope\".split(\" \"),\n",
    "    # \"I saw the man with a telescope\".split(\" \")\n",
    "])\n",
    "# q22(corpus_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef7be6e",
   "metadata": {},
   "source": [
    "### Q 2.3\n",
    "- Given a token in a sentence, write the required functions that output the dependency path from the root of the dependency tree to that given token.\n",
    "\n",
    "**Attention**: *print only the results of these functions by using the sentence \"I saw the man with a telescope\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a2fd0e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I saw the man with a telescope\n",
      "TOKEN ---> ['path', 'to', 'token']\n",
      "\n",
      "I\n",
      "\t---> ['saw', 'I']\n",
      "saw\n",
      "\t---> ['saw']\n",
      "the\n",
      "\t---> ['saw', 'man', 'the']\n",
      "man\n",
      "\t---> ['saw', 'man']\n",
      "with\n",
      "\t---> ['saw', 'with']\n",
      "a\n",
      "\t---> ['saw', 'with', 'telescope', 'a']\n",
      "telescope\n",
      "\t---> ['saw', 'with', 'telescope']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"74eddd29a20d4c75bd31d368fd7c1df8-0\" class=\"displacy\" width=\"1275\" height=\"399.5\" direction=\"ltr\" style=\"max-width: none; height: 399.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">I</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">saw</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">man</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">with</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">a</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">telescope</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-74eddd29a20d4c75bd31d368fd7c1df8-0-0\" stroke-width=\"2px\" d=\"M70,264.5 C70,177.0 215.0,177.0 215.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-74eddd29a20d4c75bd31d368fd7c1df8-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,266.5 L62,254.5 78,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-74eddd29a20d4c75bd31d368fd7c1df8-0-1\" stroke-width=\"2px\" d=\"M420,264.5 C420,177.0 565.0,177.0 565.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-74eddd29a20d4c75bd31d368fd7c1df8-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,266.5 L412,254.5 428,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-74eddd29a20d4c75bd31d368fd7c1df8-0-2\" stroke-width=\"2px\" d=\"M245,264.5 C245,89.5 570.0,89.5 570.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-74eddd29a20d4c75bd31d368fd7c1df8-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M570.0,266.5 L578.0,254.5 562.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-74eddd29a20d4c75bd31d368fd7c1df8-0-3\" stroke-width=\"2px\" d=\"M245,264.5 C245,2.0 750.0,2.0 750.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-74eddd29a20d4c75bd31d368fd7c1df8-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M750.0,266.5 L758.0,254.5 742.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-74eddd29a20d4c75bd31d368fd7c1df8-0-4\" stroke-width=\"2px\" d=\"M945,264.5 C945,177.0 1090.0,177.0 1090.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-74eddd29a20d4c75bd31d368fd7c1df8-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M945,266.5 L937,254.5 953,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-74eddd29a20d4c75bd31d368fd7c1df8-0-5\" stroke-width=\"2px\" d=\"M770,264.5 C770,89.5 1095.0,89.5 1095.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-74eddd29a20d4c75bd31d368fd7c1df8-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1095.0,266.5 L1103.0,254.5 1087.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def q23(corpus, cut=5):\n",
    "    \"\"\"\n",
    "    Wrapper function\n",
    "    Input should be a list of lists of token, e.g.:\n",
    "    [\n",
    "        [\"I\", \"saw\", \"the\", \"man\", \"with\", \"a\", \"telescope\"]\n",
    "    ]\n",
    "    \"\"\"\n",
    "    \n",
    "    def compute_dependency_path(token):\n",
    "        \"\"\"\n",
    "        returns list of nodes in the dependency tree\n",
    "        from the provided token (spacy format) and\n",
    "        the root of the sentence\n",
    "        \"\"\"\n",
    "        path = list()\n",
    "        path.append(token.text)\n",
    "        while token.dep_ != 'ROOT':\n",
    "            token = token.head\n",
    "            path.append(token.text)\n",
    "        path.reverse()\n",
    "        return path\n",
    "\n",
    "    if len(corpus) >= cut:\n",
    "        corpus = corpus[:cut]\n",
    "    sents = get_flat_sents(corpus)\n",
    "    for sent in sents:\n",
    "        print(sent)\n",
    "        doc = nlp(sent)\n",
    "        print(\"TOKEN ---> ['path', 'to', 'token']\\n\")\n",
    "        for token in doc:\n",
    "            print(\"{}\\n\\t---> {}\".format(token.text, compute_dependency_path(token)))\n",
    "\n",
    "    spacy.displacy.render(doc, style=\"dep\")\n",
    "\n",
    "q23([\"I saw the man with a telescope\".split(\" \")], 10)\n",
    "# q23(corpus_sents, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece505f0",
   "metadata": {},
   "source": [
    "## Task 3: Named Entity Recognition\n",
    "*Suggestion: use scikit-learn metric functions. See classification_report*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27df64cf",
   "metadata": {},
   "source": [
    "### Q 3.1\n",
    "- Benchmark Spacy Named Entity Recognition model on the test set by:\n",
    "    1. Providing the list of categories in the dataset (person, organization, etc.)\n",
    "    2. Computing the overall accuracy on NER\n",
    "    3. Computing the performance of the Named Entity Recognition model for each category:\n",
    "        - Compute the perfomance at the token level (eg. B-Person, I-Person, B-Organization, I-Organization, O, etc.)\n",
    "        - Compute the performance at the entity level (eg. Person, Organization, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24381fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting Spacy processed dataset\n",
    "sents_train = get_flat_sents(corpus_train.sents())\n",
    "docs_train = [nlp(sent) for sent in sents_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a6dd28fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_test = get_flat_sents(corpus_test.sents())\n",
    "docs_test = [nlp(sent) for sent in sents_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2a49d3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_all = get_flat_sents(corpus.sents())\n",
    "docs_all = [nlp(sent) for sent in sents_all]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf1d98e",
   "metadata": {},
   "source": [
    "#### Q 3.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "89f3feac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Q3.1.1]\n",
      ">\tProviding list of categories in the dataset:\n",
      ">\t Original dataset:\n",
      ">\t\t MISC LOC PER ORG\n",
      ">\t After Spacy processing:\n",
      ">\t\t LANGUAGE NORP GPE EVENT PERCENT TIME WORK_OF_ART MONEY ORG FAC PRODUCT CARDINAL ORDINAL LOC DATE QUANTITY PERSON LAW"
     ]
    }
   ],
   "source": [
    "def get_categories(corpus: ConllCorpusReader):\n",
    "    \"\"\"\n",
    "    Returns list of categories in the original dataset\n",
    "    requires to be conll2003 format\n",
    "    \"\"\"\n",
    "    categories = set()\n",
    "    for token in chain.from_iterable(corpus._grids()):\n",
    "        ent_type = token[3]\n",
    "        if ent_type != 'O':\n",
    "            categories.add(ent_type.split('-')[1])\n",
    "    return list(categories)\n",
    "\n",
    "def get_categories_spacy(docs):\n",
    "    \"\"\"\n",
    "    Returns list of categories in the spacy-processed dataset\n",
    "    \"\"\"\n",
    "    categories = set()\n",
    "    for doc in docs:\n",
    "        # doc = nlp(sent)\n",
    "        for token in doc:\n",
    "            categories.add(token.ent_type_)\n",
    "        if '' in categories:\n",
    "            categories.discard('')\n",
    "    return list(categories)\n",
    "\n",
    "print(\"[Q3.1.1]\\n>\\tProviding list of categories in the dataset:\")\n",
    "print(\">\\t Original dataset:\", end=\"\\n>\\t\\t\")\n",
    "categories = get_categories(corpus)\n",
    "for category in categories:\n",
    "    print(\" {}\".format(category), end=\"\")\n",
    "print(\"\\n>\\t After Spacy processing:\", end=\"\\n>\\t\\t\")\n",
    "categories = get_categories_spacy(docs_test)\n",
    "for category in categories:\n",
    "    print(\" {}\".format(category), end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ee198b",
   "metadata": {},
   "source": [
    "#### Q 3.1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "626062e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess dataset and spacy for classification_report usage\n",
    "def preprocess_gt(corpus):\n",
    "    \"\"\"\n",
    "    Preprocess ground truth to be a list of lists\n",
    "    of tuples (word, ent_type)\n",
    "    \"\"\"\n",
    "    WORD, _, NE = range(3)\n",
    "    gt = list()\n",
    "    for s in corpus.iob_sents():\n",
    "        gt.append([(w[WORD], w[NE]) for w in s])\n",
    "    return gt        \n",
    "        \n",
    "def preprocess_spacy(docs):  \n",
    "    \"\"\"\n",
    "    Preprocess spacy-processed dataset truth to be a \n",
    "    list of lists of tuples (word, ent_type)\n",
    "    \"\"\"\n",
    "    spacy_ner = list()\n",
    "    for doc in docs:\n",
    "        current_sent = list()\n",
    "        for token in doc:\n",
    "            ent_type = token.ent_iob_\n",
    "            if token.ent_type_ != '':\n",
    "                ent_type += f\"-{token.ent_type_}\"\n",
    "            current_sent.append((token.text, ent_type))\n",
    "        spacy_ner.append(current_sent)\n",
    "    return spacy_ner\n",
    "\n",
    "def convert_to_sk_cl_report(tuples_list):\n",
    "    \"\"\"\n",
    "    (word, ent_type) -> ent_type transforming utility\n",
    "    required by sklearn classification_report\n",
    "    \"\"\"\n",
    "    return [t[1] for t in tuples_list]\n",
    "\n",
    "gt = preprocess_gt(corpus_test)\n",
    "spacy_ner = preprocess_spacy(docs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "cb896c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Q3.1.2]\n",
      ">\tProviding overall raw accuracy: 0.7113599655432324\n"
     ]
    }
   ],
   "source": [
    "print(\"[Q3.1.2]\\n>\\tProviding overall raw accuracy:\", end=\" \")\n",
    "accuracy_raw = classification_report(\n",
    "    convert_to_sk_cl_report(chain.from_iterable(gt)), \n",
    "    convert_to_sk_cl_report(chain.from_iterable(spacy_ner)),\n",
    "    zero_division=1, output_dict=True)\n",
    "print(accuracy_raw[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "aff543d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_spacy_ents(old_spacy_ner):\n",
    "    \"\"\"\n",
    "    As the Spacy-processed data way outnumbers the category in the \n",
    "    grount truth conll2003 dataset (we saw it in 3.1.1),\n",
    "    here we define a mapping function to compute a more robust accuracy.\n",
    "\n",
    "    Mapping:\n",
    "    'spacy' -> 'conll2003'\n",
    "    GPE -> LOC\n",
    "    PERSON -> PER\n",
    "    EVENT -> MISC\n",
    "    NORP -> MISC\n",
    "    else -> O\n",
    "    \"\"\"\n",
    "    allowed = [\"LOC\", \"ORG\", \"O\"]\n",
    "    mapping_dict = {\n",
    "        \"GPE\": \"LOC\",\n",
    "        \"PERSON\": \"PER\",\n",
    "        \"EVENT\": \"MISC\",\n",
    "        \"NORP\": \"MISC\",\n",
    "    }\n",
    "    spacy_ner = list()\n",
    "    for sent in old_spacy_ner:\n",
    "        sent_temp = list()\n",
    "        for token in sent:\n",
    "            is_otag = \"-\" not in token[1]\n",
    "            ent_label = token[1].split(\"-\")[1] if not is_otag else token[1]\n",
    "            if ent_label not in allowed:\n",
    "                if ent_label in mapping_dict.keys():\n",
    "                    ent_label = mapping_dict[ent_label]\n",
    "                else:\n",
    "                    ent_label = \"O\"\n",
    "                    is_otag = True\n",
    "            if is_otag:\n",
    "                sent_temp.append((token[0], ent_label))\n",
    "            else:\n",
    "                sent_temp.append((token[0], f\"{token[1][:2]}{ent_label}\"))\n",
    "        spacy_ner.append(sent_temp)\n",
    "    return spacy_ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c6b4bdef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">\t Raw results (no mapping): \n",
      ">\t\t0.7114\n",
      ">\t Mapping spacy entity labels to grount truth: \n",
      ">\t\t0.9061\n"
     ]
    }
   ],
   "source": [
    "accuracy_mapping = classification_report(\n",
    "    convert_to_sk_cl_report(chain.from_iterable(gt)), \n",
    "    convert_to_sk_cl_report(chain.from_iterable(map_spacy_ents(spacy_ner))),\n",
    "    output_dict=True)\n",
    "\n",
    "print(\">\\t Raw results (no mapping): \\n>\\t\\t%.4f\" % accuracy_raw[\"accuracy\"])\n",
    "print(\">\\t Mapping spacy entity labels to grount truth: \\n>\\t\\t%.4f\" % accuracy_mapping[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c9e7f8",
   "metadata": {},
   "source": [
    "#### Q 3.1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2872b158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "   B-CARDINAL       0.00      1.00      0.00         0\n",
      "       B-DATE       0.00      1.00      0.00         0\n",
      "      B-EVENT       0.00      1.00      0.00         0\n",
      "        B-FAC       0.00      1.00      0.00         0\n",
      "        B-GPE       0.00      1.00      0.00         0\n",
      "   B-LANGUAGE       0.00      1.00      0.00         0\n",
      "        B-LAW       0.00      1.00      0.00         0\n",
      "        B-LOC       0.61      0.02      0.04      1668\n",
      "       B-MISC       1.00      0.00      0.00       702\n",
      "      B-MONEY       0.00      1.00      0.00         0\n",
      "       B-NORP       0.00      1.00      0.00         0\n",
      "    B-ORDINAL       0.00      1.00      0.00         0\n",
      "        B-ORG       0.49      0.28      0.36      1661\n",
      "        B-PER       1.00      0.00      0.00      1617\n",
      "    B-PERCENT       0.00      1.00      0.00         0\n",
      "     B-PERSON       0.00      1.00      0.00         0\n",
      "    B-PRODUCT       0.00      1.00      0.00         0\n",
      "   B-QUANTITY       0.00      1.00      0.00         0\n",
      "       B-TIME       0.00      1.00      0.00         0\n",
      "B-WORK_OF_ART       0.00      1.00      0.00         0\n",
      "   I-CARDINAL       0.00      1.00      0.00         0\n",
      "       I-DATE       0.00      1.00      0.00         0\n",
      "      I-EVENT       0.00      1.00      0.00         0\n",
      "        I-FAC       0.00      1.00      0.00         0\n",
      "        I-GPE       0.00      1.00      0.00         0\n",
      "        I-LAW       0.00      1.00      0.00         0\n",
      "        I-LOC       0.56      0.10      0.17       257\n",
      "       I-MISC       1.00      0.00      0.00       216\n",
      "      I-MONEY       0.00      1.00      0.00         0\n",
      "       I-NORP       0.00      1.00      0.00         0\n",
      "        I-ORG       0.46      0.51      0.49       835\n",
      "        I-PER       1.00      0.00      0.00      1156\n",
      "    I-PERCENT       0.00      1.00      0.00         0\n",
      "     I-PERSON       0.00      1.00      0.00         0\n",
      "    I-PRODUCT       0.00      1.00      0.00         0\n",
      "   I-QUANTITY       0.00      1.00      0.00         0\n",
      "       I-TIME       0.00      1.00      0.00         0\n",
      "I-WORK_OF_ART       0.00      1.00      0.00         0\n",
      "            O       0.94      0.84      0.89     38323\n",
      "\n",
      "     accuracy                           0.71     46435\n",
      "    macro avg       0.18      0.81      0.05     46435\n",
      " weighted avg       0.91      0.71      0.76     46435\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Print performances at token level without mapping\n",
    "\"\"\"\n",
    "results_t_raw = classification_report(\n",
    "    convert_to_sk_cl_report(chain.from_iterable(gt)), \n",
    "    convert_to_sk_cl_report(chain.from_iterable(spacy_ner)),\n",
    "    zero_division=1)\n",
    "print(results_t_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "fdb729e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.76      0.71      0.73      1668\n",
      "      B-MISC       0.80      0.55      0.65       702\n",
      "       B-ORG       0.49      0.28      0.36      1661\n",
      "       B-PER       0.75      0.58      0.65      1617\n",
      "       I-LOC       0.59      0.63      0.61       257\n",
      "      I-MISC       0.62      0.38      0.47       216\n",
      "       I-ORG       0.46      0.51      0.49       835\n",
      "       I-PER       0.76      0.73      0.74      1156\n",
      "           O       0.95      0.98      0.96     38323\n",
      "\n",
      "    accuracy                           0.91     46435\n",
      "   macro avg       0.69      0.59      0.63     46435\n",
      "weighted avg       0.90      0.91      0.90     46435\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Print performances at token level with mapping\n",
    "\"\"\"\n",
    "results_t_mapping = classification_report(\n",
    "    convert_to_sk_cl_report(chain.from_iterable(gt)), \n",
    "    convert_to_sk_cl_report(chain.from_iterable(map_spacy_ents(spacy_ner))))\n",
    "print(results_t_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7009b8a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    p         r         f     s\n",
      "MISC         1.000000  0.000000  0.000000   702\n",
      "LANGUAGE     0.000000  0.000000  0.000000     0\n",
      "NORP         0.000000  0.000000  0.000000     0\n",
      "GPE          0.000000  0.000000  0.000000     0\n",
      "EVENT        0.000000  0.000000  0.000000     0\n",
      "PERCENT      0.000000  0.000000  0.000000     0\n",
      "TIME         0.000000  0.000000  0.000000     0\n",
      "WORK_OF_ART  0.000000  0.000000  0.000000     0\n",
      "MONEY        0.000000  0.000000  0.000000     0\n",
      "ORG          0.439103  0.247441  0.316519  1661\n",
      "FAC          0.000000  0.000000  0.000000     0\n",
      "PRODUCT      0.000000  0.000000  0.000000     0\n",
      "CARDINAL     0.000000  0.000000  0.000000     0\n",
      "PER          1.000000  0.000000  0.000000  1617\n",
      "ORDINAL      0.000000  0.000000  0.000000     0\n",
      "DATE         0.000000  0.000000  0.000000     0\n",
      "LOC          0.564516  0.020983  0.040462  1668\n",
      "QUANTITY     0.000000  0.000000  0.000000     0\n",
      "PERSON       0.000000  0.000000  0.000000     0\n",
      "LAW          0.000000  0.000000  0.000000     0\n",
      "total        0.056299  0.078966  0.065733  5648\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Print performances at entity level without mapping\n",
    "\"\"\"\n",
    "results_e_raw = evaluate(gt, spacy_ner)\n",
    "pd_tbl = pd.DataFrame().from_dict(results_e_raw, orient='index')\n",
    "pd_tbl.round(decimals=3)\n",
    "print(pd_tbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0ec5663e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              p         r         f     s\n",
      "MISC   0.788066  0.545584  0.644781   702\n",
      "LOC    0.746461  0.695444  0.720050  1668\n",
      "PER    0.716707  0.549165  0.621849  1617\n",
      "ORG    0.439103  0.247441  0.316519  1661\n",
      "total  0.674259  0.503187  0.576295  5648\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Print performances at entity level with mapping\n",
    "\"\"\"\n",
    "results = evaluate(gt, map_spacy_ents(spacy_ner))\n",
    "pd_tbl = pd.DataFrame().from_dict(results, orient='index')\n",
    "pd_tbl.round(decimals=3)\n",
    "print(pd_tbl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226b7f98",
   "metadata": {},
   "source": [
    "## Task 4: BONUS PART (extra mark for laude)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64e4b6f",
   "metadata": {},
   "source": [
    "### Save old parser configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b346e653",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.parse.transitionparser import Configuration, TransitionParser\n",
    "old_extract_features = Configuration.extract_features\n",
    "old_train = TransitionParser.train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02c5951",
   "metadata": {},
   "source": [
    "### Retrieve treebank dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e038995f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import types\n",
    "from nltk import download\n",
    "download('dependency_treebank')\n",
    "from nltk.corpus import dependency_treebank\n",
    "from nltk.parse import DependencyEvaluator\n",
    "\n",
    "# split the dataset into train and test\n",
    "# first 100 as train dataset and last 10 as test dataset\n",
    "train_dataset = dependency_treebank.parsed_sents()[:100]\n",
    "test_dataset =  dependency_treebank.parsed_sents()[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b3cfbe",
   "metadata": {},
   "source": [
    "### Compute performance of the original parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6848910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Number of training examples : 100\n",
      " Number of valid (projective) examples : 100\n",
      "original labelled attachment score  0.7791666666666667\n",
      "original unlabelled attachment score 0.7791666666666667\n"
     ]
    }
   ],
   "source": [
    "Configuration.extract_features = old_extract_features\n",
    "tp = TransitionParser('arc-standard')\n",
    "tp.train(train_dataset, 'tp.model', verbose=False)\n",
    "\n",
    "# parsing takes a list of dependency graphs and a model as arguments\n",
    "parses = tp.parse(test_dataset, 'tp.model')\n",
    "\n",
    "# evaluating the parser\n",
    "de = DependencyEvaluator(parses, test_dataset)\n",
    "las, uas = de.eval()\n",
    "\n",
    "# no labels, thus identical\n",
    "print(\"original labelled attachment score \",las)\n",
    "print(\"original unlabelled attachment score\",uas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7080256",
   "metadata": {},
   "source": [
    "### Q 4.1\n",
    "- Modify NLTK Transition parser's Configuration calss to use better features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f1a75ea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['STK_0_POS_TOP',\n",
       " 'STK_0_CTAG_TOP',\n",
       " 'STK_0_LDEPS_1',\n",
       " 'BUF_0_FORM_Economic',\n",
       " 'BUF_0_LEMMA_Economic',\n",
       " 'BUF_0_POS_JJ',\n",
       " 'BUF_0_CTAG_JJ',\n",
       " 'BUF_0_HEAD_2',\n",
       " 'BUF_0_LDEPS_0',\n",
       " 'BUF_0_REL_ATT',\n",
       " 'BUF_0_LFEATS_0',\n",
       " 'BUF_1_LEMMA_news',\n",
       " 'BUF_1_POS_NN',\n",
       " 'BUF_2_POS_VBD',\n",
       " 'BUF_3_POS_JJ',\n",
       " 'BUF_4_POS_NN']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.parse import DependencyGraph\n",
    "def new_extract_features(self):\n",
    "    \"\"\"\n",
    "    Extract the set of features for the current configuration. Implement standard features as describe in\n",
    "    Table 3.2 (page 31) in Dependency Parsing book by Sandra Kubler, Ryan McDonal, Joakim Nivre.\n",
    "    Please note that these features are very basic.\n",
    "    \n",
    "    MODIFIED\n",
    "    The simplest and quickest improvement I found was to make use of all the\n",
    "    fields in the token dict (ctag, head) and some metadata of the nested\n",
    "    structures (e.g. the length of the deps and feats fields)\n",
    "\n",
    "    It turns out to be effective mostly if done at first token in both\n",
    "    stack and buffer; when I added more information in 1, 2, etc performances\n",
    "    significantly lowered\n",
    "\n",
    "    Overall this result in a gain of 10% in the performances\n",
    "\n",
    "    :return: list(str)\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    # print(\"n\", end=\" \")\n",
    "    if len(self.stack) > 0:\n",
    "        # Stack 0\n",
    "        stack_idx0 = self.stack[len(self.stack) - 1]\n",
    "        token = self._tokens[stack_idx0]\n",
    "        if self._check_informative(token[\"word\"], True):\n",
    "            result.append(\"STK_0_FORM_\" + token[\"word\"])\n",
    "        if \"lemma\" in token and self._check_informative(token[\"lemma\"]):\n",
    "            result.append(\"STK_0_LEMMA_\" + token[\"lemma\"])\n",
    "        if self._check_informative(token[\"tag\"]):\n",
    "            result.append(\"STK_0_POS_\" + token[\"tag\"])\n",
    "        if self._check_informative(token[\"ctag\"], True):\n",
    "            result.append(\"STK_0_CTAG_\" + token[\"ctag\"])\n",
    "        if self._check_informative(token[\"head\"], True):\n",
    "            result.append(\"STK_0_HEAD_\" + str(token[\"head\"]))\n",
    "        if self._check_informative(len(token[\"deps\"]), True):\n",
    "            result.append(\"STK_0_LDEPS_\" + str(len(token[\"deps\"])))\n",
    "        if self._check_informative(token[\"rel\"]):\n",
    "            result.append(\"STK_0_REL_\" + token[\"rel\"])\n",
    "        if \"feats\" in token:\n",
    "            if token[\"feats\"] is not None and self._check_informative(len(token[\"feats\"])):\n",
    "                result.append(\"STK_0_LFEATS_\" + str(len(token[\"feats\"])))\n",
    "            if self._check_informative(token[\"feats\"]):\n",
    "                feats = token[\"feats\"].split(\"|\")\n",
    "                for feat in feats:\n",
    "                    result.append(\"STK_0_FEATS_\" + feat)\n",
    "        # Stack 1\n",
    "        if len(self.stack) > 1:\n",
    "            stack_idx1 = self.stack[len(self.stack) - 2]\n",
    "            token = self._tokens[stack_idx1]\n",
    "            if self._check_informative(token[\"tag\"]):\n",
    "                result.append(\"STK_1_POS_\" + token[\"tag\"])\n",
    "            if self._check_informative(token[\"lemma\"], True):\n",
    "                result.append(\"STK_1_LEMMA_\" + token[\"lemma\"])\n",
    "\n",
    "        # Left most, right most dependency of stack[0]\n",
    "        left_most = 1000000\n",
    "        right_most = -1\n",
    "        dep_left_most = \"\"\n",
    "        dep_right_most = \"\"\n",
    "        for (wi, r, wj) in self.arcs:\n",
    "            if wi == stack_idx0:\n",
    "                if (wj > wi) and (wj > right_most):\n",
    "                    right_most = wj\n",
    "                    dep_right_most = r\n",
    "                if (wj < wi) and (wj < left_most):\n",
    "                    left_most = wj\n",
    "                    dep_left_most = r\n",
    "        if self._check_informative(dep_left_most):\n",
    "            result.append(\"STK_0_LDEP_\" + dep_left_most)\n",
    "        if self._check_informative(dep_right_most):\n",
    "            result.append(\"STK_0_RDEP_\" + dep_right_most)\n",
    "\n",
    "    # Check Buffered 0\n",
    "    if len(self.buffer) > 0:\n",
    "        # Buffer 0\n",
    "        buffer_idx0 = self.buffer[0]\n",
    "        token = self._tokens[buffer_idx0]\n",
    "        if self._check_informative(token[\"word\"], True):\n",
    "            result.append(\"BUF_0_FORM_\" + token[\"word\"])\n",
    "        if \"lemma\" in token and self._check_informative(token[\"lemma\"]):\n",
    "            result.append(\"BUF_0_LEMMA_\" + token[\"lemma\"])\n",
    "        if self._check_informative(token[\"tag\"]):\n",
    "            result.append(\"BUF_0_POS_\" + token[\"tag\"])\n",
    "        if self._check_informative(token[\"ctag\"]):\n",
    "            result.append(\"BUF_0_CTAG_\" + token[\"ctag\"])\n",
    "        if self._check_informative(token[\"head\"]):\n",
    "            result.append(\"BUF_0_HEAD_\" + str(token[\"head\"]))\n",
    "        if self._check_informative(len(token[\"deps\"]), True):\n",
    "            result.append(\"BUF_0_LDEPS_\" + str(len(token[\"deps\"])))\n",
    "        if self._check_informative(token[\"rel\"]):\n",
    "            result.append(\"BUF_0_REL_\" + token[\"rel\"])\n",
    "        if \"feats\" in token:\n",
    "            if token[\"feats\"] is not None and self._check_informative(len(token[\"feats\"])):\n",
    "                result.append(\"BUF_0_LFEATS_\" + str(len(token[\"feats\"])))\n",
    "            if self._check_informative(token[\"feats\"]):\n",
    "                feats = token[\"feats\"].split(\"|\")\n",
    "                for feat in feats:\n",
    "                    result.append(\"BUF_0_FEATS_\" + feat)\n",
    "        # Buffer 1\n",
    "        if len(self.buffer) > 1:\n",
    "            buffer_idx1 = self.buffer[1]\n",
    "            token = self._tokens[buffer_idx1]\n",
    "            ### DISCARDED performance lowered\n",
    "            # if self._check_informative(token[\"word\"], True):\n",
    "            #     result.append(\"BUF_1_FORM_\" + token[\"word\"])\n",
    "            # if self._check_informative(token[\"head\"]):\n",
    "            #     result.append(\"BUF_1_HEAD_\" + str(token[\"head\"]))\n",
    "            if self._check_informative(token[\"lemma\"], True):\n",
    "                result.append(\"BUF_1_LEMMA_\" + token[\"lemma\"])\n",
    "            if self._check_informative(token[\"tag\"]):\n",
    "                result.append(\"BUF_1_POS_\" + token[\"tag\"])\n",
    "        if len(self.buffer) > 2:\n",
    "            buffer_idx2 = self.buffer[2]\n",
    "            token = self._tokens[buffer_idx2]\n",
    "            if self._check_informative(token[\"tag\"]):\n",
    "                result.append(\"BUF_2_POS_\" + token[\"tag\"])\n",
    "        if len(self.buffer) > 3:\n",
    "            buffer_idx3 = self.buffer[3]\n",
    "            token = self._tokens[buffer_idx3]\n",
    "            if self._check_informative(token[\"tag\"]):\n",
    "                result.append(\"BUF_3_POS_\" + token[\"tag\"])\n",
    "        if len(self.buffer) > 4:\n",
    "            buffer_idx4 = self.buffer[4]\n",
    "            token = self._tokens[buffer_idx4]\n",
    "            if self._check_informative(token[\"tag\"]):\n",
    "                result.append(\"BUF_4_POS_\" + token[\"tag\"])\n",
    "                # Left most, right most dependency of stack[0]\n",
    "        left_most = 1000000\n",
    "        right_most = -1\n",
    "        dep_left_most = \"\"\n",
    "        dep_right_most = \"\"\n",
    "        for (wi, r, wj) in self.arcs:\n",
    "            if wi == buffer_idx0:\n",
    "                if (wj > wi) and (wj > right_most):\n",
    "                    right_most = wj\n",
    "                    dep_right_most = r\n",
    "                if (wj < wi) and (wj < left_most):\n",
    "                    left_most = wj\n",
    "                    dep_left_most = r\n",
    "        if self._check_informative(dep_left_most):\n",
    "            result.append(\"BUF_0_LDEP_\" + dep_left_most)\n",
    "        if self._check_informative(dep_right_most):\n",
    "            result.append(\"BUF_0_RDEP_\" + dep_right_most)\n",
    "\n",
    "    return result\n",
    "\n",
    "Configuration.extract_features = new_extract_features\n",
    "\n",
    "\n",
    "gold_sent = DependencyGraph(\"\"\"\n",
    "Economic  JJ     2      ATT\n",
    "news  NN     3       SBJ\n",
    "has       VBD       0       ROOT\n",
    "little      JJ      5       ATT\n",
    "effect   NN     3       OBJ\n",
    "on     IN      5       ATT\n",
    "financial       JJ       8       ATT\n",
    "markets    NNS      6       PC\n",
    ".    .      3       PU\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "conf = Configuration(gold_sent)\n",
    "conf.extract_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406bd578",
   "metadata": {},
   "source": [
    "### Q 4.2\n",
    "- Evaluate the features comparing performance to the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fe9c9951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Number of training examples : 100\n",
      " Number of valid (projective) examples : 100\n",
      "\n",
      "original labelled attachment score  0.7791666666666667\n",
      "modified feature extractor labelled attachment score 0.875\n",
      "\n",
      "original unlabelled attachment score 0.7791666666666667\n",
      "modified feature extractor unlabelled attachment score 0.875\n"
     ]
    }
   ],
   "source": [
    "Configuration.extract_features = new_extract_features\n",
    "# using the TransitionParser \n",
    "tp_new_fe = TransitionParser('arc-standard')\n",
    "tp_new_fe.train(train_dataset, 'tp_new_fe.model', verbose=False)\n",
    "\n",
    "# parsing takes a list of dependency graphs and a model as arguments\n",
    "parses_new_fe = tp_new_fe.parse(test_dataset, 'tp_new_fe.model')\n",
    "\n",
    "de_new_fe = DependencyEvaluator(parses_new_fe, test_dataset)\n",
    "las_new_fe, uas_new_fe = de_new_fe.eval()\n",
    "\n",
    "print(\"\\noriginal labelled attachment score \",las)\n",
    "print('modified feature extractor labelled attachment score',las_new_fe, end=\"\\n\\n\")\n",
    "print(\"original unlabelled attachment score\",uas)\n",
    "print('modified feature extractor unlabelled attachment score',uas_new_fe)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1408d421",
   "metadata": {},
   "source": [
    "### Q 4.3\n",
    "- Replace SVM classifier with an alternative of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6ab09524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LINEAR CLASSIFIER\n",
      "\n",
      " Number of training examples : 100\n",
      " Number of valid (projective) examples : 100\n",
      "modified classifier labelled attachment score 0.8291666666666667\n",
      "modified classifier unlabelled attachment score 0.8291666666666667\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Replace with a SGD linear classifier\n",
    "\"\"\"\n",
    "import pickle\n",
    "import tempfile\n",
    "try:\n",
    "    from sklearn.linear_model import SGDClassifier\n",
    "    from sklearn.datasets import load_svmlight_file\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "def sgd_train(self, depgraphs, modelfile, verbose=True):\n",
    "    \"\"\"\n",
    "    :param depgraphs : list of DependencyGraph as the training data\n",
    "    :type depgraphs : DependencyGraph\n",
    "    :param modelfile : file name to save the trained model\n",
    "    :type modelfile : str\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        input_file = tempfile.NamedTemporaryFile(\n",
    "            prefix=\"transition_parse.train\", dir=tempfile.gettempdir(), delete=False\n",
    "        )\n",
    "\n",
    "        if self._algorithm == self.ARC_STANDARD:\n",
    "            self._create_training_examples_arc_std(depgraphs, input_file)\n",
    "        else:\n",
    "            self._create_training_examples_arc_eager(depgraphs, input_file)\n",
    "\n",
    "        input_file.close()\n",
    "        # Using the temporary file to train the libsvm classifier\n",
    "        x_train, y_train = load_svmlight_file(input_file.name)\n",
    "\n",
    "        model = SGDClassifier(\n",
    "            loss=\"log\",\n",
    "            penalty=\"l2\",\n",
    "            shuffle=True,\n",
    "            verbose=0,\n",
    "            learning_rate=\"optimal\"\n",
    "        )\n",
    "        model.fit(x_train, y_train)\n",
    "        # Save the model to file name (as pickle)\n",
    "        pickle.dump(model, open(modelfile, \"wb\"))\n",
    "    finally:\n",
    "        os.remove(input_file.name)\n",
    "\n",
    "print(\"LINEAR CLASSIFIER\\n\")\n",
    "tp_sgd_clf = TransitionParser('arc-standard')\n",
    "tp_sgd_clf.train = types.MethodType(sgd_train,tp_sgd_clf)\n",
    "tp_sgd_clf.train(train_dataset, 'tp_sgd_clf.model', verbose=False)\n",
    "parses_sgd_clf = tp_sgd_clf.parse(test_dataset, 'tp_sgd_clf.model')\n",
    "de_sgd_clf = DependencyEvaluator(parses_sgd_clf, test_dataset)\n",
    "las_sgd_clf, uas_sgd_clf = de_sgd_clf.eval()\n",
    "print('modified classifier labelled attachment score',las_sgd_clf)\n",
    "print('modified classifier unlabelled attachment score',uas_sgd_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e3f0049a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RANDOM FOREST CLASSIFIER\n",
      "\n",
      " Number of training examples : 100\n",
      " Number of valid (projective) examples : 100\n",
      "modified classifier labelled attachment score 0.8\n",
      "modified classifier unlabelled attachment score 0.8\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Replace with a Random Forest classifier\n",
    "\"\"\"\n",
    "import pickle\n",
    "import tempfile\n",
    "try:\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.datasets import load_svmlight_file\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "def rf_train(self, depgraphs, modelfile, verbose=True):\n",
    "    \"\"\"\n",
    "    :param depgraphs : list of DependencyGraph as the training data\n",
    "    :type depgraphs : DependencyGraph\n",
    "    :param modelfile : file name to save the trained model\n",
    "    :type modelfile : str\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        input_file = tempfile.NamedTemporaryFile(\n",
    "            prefix=\"transition_parse.train\", dir=tempfile.gettempdir(), delete=False\n",
    "        )\n",
    "\n",
    "        if self._algorithm == self.ARC_STANDARD:\n",
    "            self._create_training_examples_arc_std(depgraphs, input_file)\n",
    "        else:\n",
    "            self._create_training_examples_arc_eager(depgraphs, input_file)\n",
    "\n",
    "        input_file.close()\n",
    "        # Using the temporary file to train the libsvm classifier\n",
    "        x_train, y_train = load_svmlight_file(input_file.name)\n",
    "\n",
    "        model = RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            criterion=\"entropy\",\n",
    "            max_features=\"log2\"\n",
    "        )\n",
    "        \n",
    "        model.fit(x_train, y_train)\n",
    "        # Save the model to file name (as pickle)\n",
    "        pickle.dump(model, open(modelfile, \"wb\"))\n",
    "    finally:\n",
    "        os.remove(input_file.name)\n",
    "\n",
    "print(\"RANDOM FOREST CLASSIFIER\\n\")\n",
    "tp_rf_clf = TransitionParser('arc-standard')\n",
    "tp_rf_clf.train = types.MethodType(rf_train,tp_rf_clf)\n",
    "tp_rf_clf.train(train_dataset, 'tp_rf_clf.model', verbose=False)\n",
    "parses_rf_clf = tp_rf_clf.parse(test_dataset, 'tp_rf_clf.model')\n",
    "de_rf_clf = DependencyEvaluator(parses_rf_clf, test_dataset)\n",
    "las_rf_clf, uas_rf_clf = de_rf_clf.eval()\n",
    "print('modified classifier labelled attachment score',las_rf_clf)\n",
    "print('modified classifier unlabelled attachment score',uas_rf_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dea0c90b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP CLASSIFIER\n",
      "\n",
      " Number of training examples : 100\n",
      " Number of valid (projective) examples : 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pips/Projects/NLU-UniTN-2022/venv/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified classifier labelled attachment score 0.8083333333333333\n",
      "modified classifier unlabelled attachment score 0.8083333333333333\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Replace with a Multi-Layer Perceptron nn classifier\n",
    "\"\"\"\n",
    "import pickle\n",
    "import tempfile\n",
    "try:\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from sklearn.datasets import load_svmlight_file\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "def mlp_train(self, depgraphs, modelfile, verbose=True):\n",
    "    \"\"\"\n",
    "    :param depgraphs : list of DependencyGraph as the training data\n",
    "    :type depgraphs : DependencyGraph\n",
    "    :param modelfile : file name to save the trained model\n",
    "    :type modelfile : str\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        input_file = tempfile.NamedTemporaryFile(\n",
    "            prefix=\"transition_parse.train\", dir=tempfile.gettempdir(), delete=False\n",
    "        )\n",
    "\n",
    "        if self._algorithm == self.ARC_STANDARD:\n",
    "            self._create_training_examples_arc_std(depgraphs, input_file)\n",
    "        else:\n",
    "            self._create_training_examples_arc_eager(depgraphs, input_file)\n",
    "\n",
    "        input_file.close()\n",
    "        # Using the temporary file to train the libsvm classifier\n",
    "        x_train, y_train = load_svmlight_file(input_file.name)\n",
    "        \n",
    "        model = MLPClassifier(\n",
    "            activation=\"relu\", \n",
    "            learning_rate=\"adaptive\",\n",
    "            solver='sgd', \n",
    "            nesterovs_momentum=True,\n",
    "            alpha=1e-5,\n",
    "            hidden_layer_sizes=(20,50,20), \n",
    "            random_state=1\n",
    "            )\n",
    "\n",
    "\n",
    "        model.fit(x_train, y_train)\n",
    "        # Save the model to file name (as pickle)\n",
    "        pickle.dump(model, open(modelfile, \"wb\"))\n",
    "    finally:\n",
    "        os.remove(input_file.name)\n",
    "\n",
    "print(\"MLP CLASSIFIER\\n\")\n",
    "tp_mlp_clf = TransitionParser('arc-standard')\n",
    "tp_mlp_clf.train = types.MethodType(mlp_train,tp_mlp_clf)\n",
    "tp_mlp_clf.train(train_dataset, 'tp_mlp_clf.model', verbose=False)\n",
    "parses_mlp_clf = tp_mlp_clf.parse(test_dataset, 'tp_mlp_clf.model')\n",
    "de_mlp_clf = DependencyEvaluator(parses_mlp_clf, test_dataset)\n",
    "las_mlp_clf, uas_mlp_clf = de_mlp_clf.eval()\n",
    "print('modified classifier labelled attachment score',las_mlp_clf)\n",
    "print('modified classifier unlabelled attachment score',uas_mlp_clf)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b8ea92f07f1cce96500be7f9231af4df2b17add51555df0958a949f03550b4d9"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
