\section{Conclusions}
In this study we demonstrated that LSTM-based language models can benefit greatly from the usage of dropout techniques, as well as from an increased model size. We also showed that partially shuffling the training sentences can help in reducing overfitting, at the expense of slightly higher perplexity. In addition, we inspected the model behaviour and discovered that the best performing model still heavily relies on the most frequent token in the dataset, and it has inconsistent behaviour with longer sentences. We also showed that the model is effective in learning to compose words so that they comply to common phrasings such as numbers. Furthermore, the model is able to generate syntactically correct and somewhat meaningful sentences, as long as they are not too long. Finally, we created a TUI application to easily interact with the model and generate sentences.