\section{Task Formalisation}
% approx. 200 words
Language modelling is the task of developing a model which is able to predict which word comes next in a sequence. 

A \emph{language model} is a probablity distribution over sequences of words. Suppose we have a sequence of \(n\) words modeled as a joint probability: \(P(w^{(n)}) = P(w_1, \ldots, w_n)\); we can compute this probablity using the \emph{chain rule}, which comes from the Bayes rule: \(P(A, B) = P(A|B)P(B)\). Ultimately, a language model computes the probability of a sequence as follows:

\begin{equation}
    P(w_1, \ldots, w_n)  = \prod_{k=1}^{n}P \parenth[\bigg]{w_k|w_1^{(k-1)}}
\end{equation}

The underlying assumption is that sentences in natural languages have \emph{structure}, hence we can compute the joint distribution from data, i.e. \emph{corpora}. However, this setting is challenged by several properties of natural languages. Although we can trace some general schemes for structure - e.g. word orders like VSO, SVO, \emph{et cetera} - natural languages allow non-standard phrasing; moreover, they allow sentences to grow up to arbitrary lengths. More generally, it is not possible to compute statistics for the sentences space, which has to be considered unbounded in the scope of this task.

To cope with this problem, we can compute statistics for a word considering a small window of preceeding words. Formally, motivated by the Markov Assumption, we think of this process as mapping the original space into a smaller one, for which we can easily compute statistics: these language models are called \emph{n}-gram language models, and they usually consider bigram probabilities - \(P \parenth[\big]{w_i | w_1^{(i-1)}} \approx P \parenth[\big]{w_i | w_{i-1}}\) - or trigram probabilities - \(P\parenth[\big]{w_i | w_1^{(i-1)}} \approx P \parenth[\big]{w_i | w_{i-2}^{(i-1)}}\).

Currently, state-of-the-art language models are \emph{Neural language models}, i.e. language models that use neural network models to perform embeddings and approximate the distribution. In particular, the first effective models to be employed where Recurrent Neural Networks, such as LSTMs and GRUs; last developments revolve around attention-based models\cite{vaswani2023attention}, and specifically Transformers such as BERT\cite{devlin2018bert} and GPT\cite{radford2019Language2}\cite{brown2020language3}. 
