%% Created for Prasanta Ghosh on -01-20


@misc{gal2016theoretically,
      title={A Theoretically Grounded Application of Dropout in Recurrent Neural Networks}, 
      author={Yarin Gal and Zoubin Ghahramani},
      year={2016},
      eprint={1512.05287},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{merity2017regularizing,
      title={Regularizing and Optimizing LSTM Language Models}, 
      author={Stephen Merity and Nitish Shirish Keskar and Richard Socher},
      year={2017},
      eprint={1708.02182},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{vaswani2023attention,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2023},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@InProceedings{wan2013dropconnect,
  title = 	 {Regularization of Neural Networks using DropConnect},
  author = 	 {Wan, Li and Zeiler, Matthew and Zhang, Sixin and Le Cun, Yann and Fergus, Rob},
  booktitle = 	 {Proceedings of the 30th International Conference on Machine Learning},
  pages = 	 {1058--1066},
  year = 	 {2013},
  editor = 	 {Dasgupta, Sanjoy and McAllester, David},
  volume = 	 {28},
  number =       {3},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Atlanta, Georgia, USA},
  month = 	 {17--19 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v28/wan13.pdf},
  url = 	 {https://proceedings.mlr.press/v28/wan13.html},
  abstract = 	 {We introduce DropConnect, a generalization of DropOut, for regularizing large fully-connected layers within neural networks. When training with Dropout, a randomly selected subset of activations are set to zero within each layer. DropConnect instead sets a randomly selected subset of weights within the network to zero. Each unit thus receives input from a random subset of units in the previous layer. We derive a bound on the generalization performance of both Dropout and DropConnect. We then evaluate DropConnect on a range of datasets, comparing to Dropout, and show state-of-the-art results on several image recoginition benchmarks can be obtained by aggregating multiple DropConnect-trained models.}
}

@misc{press2017tying,
      title={Using the Output Embedding to Improve Language Models}, 
      author={Ofir Press and Lior Wolf},
      year={2017},
      eprint={1608.05859},
      archivePrefix={arXiv}
}

@misc{press2019partially,
      title={Partially Shuffling the Training Data to Improve Language Models}, 
      author={Ofir Press},
      year={2019},
      eprint={1903.04167},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}